{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f896a5b3",
   "metadata": {},
   "source": [
    "# Weighted Averages and Measurement Uncertainties\n",
    "\n",
    "### Goals:\n",
    "\n",
    "1. To review simple statistics, the mean and standard deviation.\n",
    "2. To understand a new statistic, the variance, and why it is so useful.\n",
    "3. To understand why using the inverse of the variance to do weighted averages, (i,e., \"inverse variance weighting\") is the standard way to combine measurements that have different uncertainties.\n",
    "\n",
    "### Timing\n",
    "\n",
    "1. Try to finish this notebook in 30-35 minutes\n",
    "\n",
    "### Question and Answer Template\n",
    "\n",
    "You can go to the link below, and do \"file\" -> \"make a copy\" to make yourself a google doc that you can use to fill in the answers to the question in this weeks notebooks.\n",
    "\n",
    "https://docs.google.com/document/d/1uNPXYCd6IF-jAnPyq7k9pFP2x7VIohZZstGqm6t0WC8/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13e6f6",
   "metadata": {},
   "source": [
    "### New functions we will use in this module\n",
    "\n",
    "| Function Name            | What it does |\n",
    "| - | - |\n",
    "| numpy.var                | Compute the variance of the values in an array |\n",
    "| numpy.random.normal      | Generate random numbers from a normal or 'Gaussian' distribution |\n",
    "| array.size               | return the number of elements in an array |\n",
    "| array.shape              | return the shape of an array, i.e., arrays have more that one dimension and this function tells you the shape of the array.  The size of the array is the product of the size of all the axes of the array |\n",
    "| plt.errorbar             | make a plot with error bars |\n",
    "| plt.legend               | add a legend to a figure |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f3fdd",
   "metadata": {},
   "source": [
    "### Formulas\n",
    "\n",
    "mean = $\\frac{\\sum_i x_i}{n}$\n",
    "\n",
    "weighted mean = $\\frac{\\sum_i w_i * x_i}{\\sum w_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e139f9f",
   "metadata": {},
   "source": [
    "# Loading Hubble measurement results\n",
    "\n",
    "Let's re-use the cell from last week where we loaded the Hubble measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437579ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Hubble.txt\", 'rb'), usecols=[1,2,3])\n",
    "# This is how we pull out the data from columns in the array.\n",
    "H0_measured = data[:,0]\n",
    "H0_errorLow = data[:,1]\n",
    "H0_errorHigh = data[:,2]\n",
    "N_measurements = H0_measured.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911cd4a",
   "metadata": {},
   "source": [
    "#### Histogramming the results\n",
    "\n",
    "Here is a histogram of the results.  Note that we have expanded the x-axis.  You will see why in a few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(H0_measured, bins=np.linspace(55.5, 90.5, 45))\n",
    "plt.xlabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "plt.ylabel(\"Counts [per 1.0 km/s/Mpc]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfa188",
   "metadata": {},
   "source": [
    "### Adding measurement errors.\n",
    "\n",
    "Now let's plot the measurements with error bars to visualize the uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15784b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(H0_measured, np.arange(N_measurements), xerr=(H0_errorLow, H0_errorHigh), fmt=\".\")\n",
    "plt.xlabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "plt.ylabel(\"Experiment number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e344a",
   "metadata": {},
   "source": [
    "### Questions for discussion\n",
    "\n",
    "#### 1.1 What do we learn from the figure with the error bars included as opposed to the histogram?  Does it change your estimate of what the true value of the Hubble parameter is?  \n",
    "\n",
    "#### 1.2 There is no single true value that is consistent with all (or even most) of the measured values within the errors.  I.e., there is no value along the x-axis that we could draw a vertical line from that would go through all the error bars.  What do you think that means?  \n",
    "\n",
    "#### 1.3 Before proceeding, describe a way that you might derive an estimate of the true value of the Hubble parameter (and an uncertainty on that value) that uses both the measured values and their stated uncertainties.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90775627",
   "metadata": {},
   "source": [
    "## Review from last week: quantifying the scatter of data.\n",
    "\n",
    "Recall that last week we explored a number of different ways to quantify the scatter in data.\n",
    "\n",
    "The last two that we discussed were the \"standard deviation\" and \"standard error\".\n",
    "\n",
    "The \"standard deviation\" tells use how much intrisict scatter there is in the data. It is equivalent to the RMS (root-mean-square) of a dataset with a mean of 0.\n",
    "\n",
    "The \"standard error\" accounts for the fact that if we keep measuring something, and the measurements keep falling within the same range, that actually gives us a better estimate of what the true value is, because we are averaging out the statistical fluctuations. It is also known as the standard error on the mean because it quantifies the uncertainty on the calculated mean of a large number of measurements.\n",
    "\n",
    "The formula for these are:\n",
    "\n",
    "average:            $ \\mu = \\frac{\\sum_i x_i}{n}$\n",
    "\n",
    "standard deviation: $ \\sigma = (\\frac{\\sum_i (x_i - \\mu)^2}{n})^{1/2} $\n",
    "\n",
    "standard error:     $ \\frac{\\sigma}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dc6f4",
   "metadata": {},
   "source": [
    "## The \"variance\", an extremely useful quantity.  \n",
    "\n",
    "The \"variance\" of a distribution is the square of the standard deviation, and it has a number of useful properties that we will explore later in this course.  For now I'm just going to write down the equation.\n",
    "\n",
    "variance : $\\sigma^2 = \\frac{\\sum_i (x_i - \\mu)^2}{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7377ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = np.var(H0_measured)\n",
    "variance_check = np.std(H0_measured)**2\n",
    "print(\"Variance:       \", variance)\n",
    "print(\"Variance check: \", variance_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will throw an error if these values aren't close!\n",
    "assert np.isclose(variance, variance_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd8a5a",
   "metadata": {},
   "source": [
    "# Simulating data from a bell curve \n",
    "\n",
    "## (a.k.a. a \"Gaussian\" or \"Normal\" distribution)\n",
    "\n",
    "We will be discussing the \"Gaussian\" or \"Normal\" distribution a lot more in coming weeks.  \n",
    "\n",
    "For now let's just state that in many cases a normal distribution is a good representation the sort of random variations you expect to see in data if you perform a measurement many times. It is parameterized by μ and σ, which represent the mean and the standard deviation of the data. The [wikipedia page](https://en.wikipedia.org/wiki/Normal_distribution) gives a good summary.\n",
    "\n",
    "The function `rng.normal` will generate values from a Gaussian distribution. Let's generate a Gaussian with $\\mu$ equal to mean and $\\sigma$ equal to the standard deviation of the Hubble measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42) # choose 42 as seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2def042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(rng.normal) # uncomment for help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_measurements = len(H0_measured)\n",
    "H0_mean = np.mean(H0_measured)\n",
    "H0_std = np.std(H0_measured)\n",
    "H0_err = H0_std / np.sqrt(N_measurements)\n",
    "\n",
    "# This prints values using python's f-strings\n",
    "print(f\"H0 = {H0_mean:0.3f} ± {H0_err:0.3f}\")  # use :0.3f to get 3 decimals from the float number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8596447",
   "metadata": {},
   "outputs": [],
   "source": [
    "simData = np.random.normal(loc=H0_mean, scale=H0_std, size=100000)\n",
    "plt.hist(simData, bins=np.linspace(55., 85., 121))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640040b",
   "metadata": {},
   "source": [
    "### Simulating a bunch of measurements of the Hubble constant\n",
    "\n",
    "Now we are going to pretend that we sent out 5 teams of scientists, and asked each of them to do some measurements of the Hubble constant, and that **all the measurements are drawn from the Gaussian distribution above**.  Each team performs a different number of measurements, but in total they have 100 measurements.\n",
    "\n",
    "We are then going to consider two different ways of combining their results.\n",
    "\n",
    "1. Taking the overall mean and error on the mean from all 100 measurements.\n",
    "2. Taking the mean and the error on the mean from each group of scientists, and making a weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSample_0 = rng.normal(loc=H0_mean, scale=H0_std, size=50)\n",
    "dataSample_1 = rng.normal(loc=H0_mean, scale=H0_std, size=3)\n",
    "dataSample_2 = rng.normal(loc=H0_mean, scale=H0_std, size=27)\n",
    "dataSample_3 = rng.normal(loc=H0_mean, scale=H0_std, size=2)\n",
    "dataSample_4 = rng.normal(loc=H0_mean, scale=H0_std, size=18)\n",
    "dataSamples = [dataSample_0, dataSample_1, dataSample_2, dataSample_3, dataSample_4]\n",
    "mergedSample = np.hstack(dataSamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b237c",
   "metadata": {},
   "source": [
    "### Now we are going to use numpy to do some statistics for us\n",
    "\n",
    "Pro-tip: python gives you a shortcut for writing out loops\n",
    "\n",
    "for example `[np.mean(dataSample) for dataSample in dataSamples]` tells python \n",
    "\n",
    "1. loop over all the elements in the list of dataSamples\n",
    "2. for each element to take the mean\n",
    "3. add that mean onto a list\n",
    "\n",
    "This construction is called \"list comprehension\" and is used a lot by python programmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([np.mean(dataSample) for dataSample in dataSamples])\n",
    "stds = np.array([np.std(dataSample) for dataSample in dataSamples])\n",
    "errors = np.array([np.std(dataSample)/np.sqrt(len(dataSample)) for dataSample in dataSamples])\n",
    "for mean, err in zip(means, errors):\n",
    "    print(f\"Value = {mean:0.2f} ± {err:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd4d92",
   "metadata": {},
   "source": [
    "### Now let's plot the summary results that each group of scientists might report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5faef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(means, np.arange(len(means)), xerr=(errors), fmt=\".\")\n",
    "plt.xlim(67.,76.)\n",
    "plt.xlabel(\"Mean of sub-sample\")\n",
    "plt.ylabel(\"Group number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260cc33",
   "metadata": {},
   "source": [
    "### Questions for discussion: \n",
    "\n",
    "#### 2.1 Even though all of the measurements are being simulated from the same distribution, the different groups of scientists are reporting different uncertainties for their best estimates.  Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c4c55",
   "metadata": {},
   "source": [
    "### Combining the scientists' measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f0497d",
   "metadata": {},
   "source": [
    "If all of the scientists were open with their data, we could just aggregate the data and calculate the mean and error using the aggregate sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean  = np.mean(mergedSample)\n",
    "overall_std   = np.std(mergedSample)\n",
    "overall_error = np.std(mergedSample) / np.sqrt(len(mergedSample))\n",
    "print(f\"Mean:  {overall_mean:0.3f} ± {overall_error:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36444bf0",
   "metadata": {},
   "source": [
    "In science, often different groups of scientists measure the same parameter through different approaches/instruments, or don't directly work together. So in this case, it's not straightforward to combine all of the measurements and take the mean. Instead, we only have the mean and error reported from each group. \n",
    "\n",
    "Let's combine the reports from the scientists in two ways: \n",
    "- using a \"straightforward\" mean (i.e. the formula that we have been using for the mean), and\n",
    "- using a weighted mean, where each mean is divided by the variance of that measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "straight_mean = np.mean(means)\n",
    "straight_error = np.std(means)/np.sqrt(len(means))\n",
    "\n",
    "weights        = 1./(errors*errors)\n",
    "weighted_mean  = np.sum(means*weights)/np.sum(weights)\n",
    "weighted_error = np.sqrt(1/np.sum(weights))\n",
    "\n",
    "print(f\"Straight mean: {straight_mean:0.3f} ± {straight_error:0.3f}\")\n",
    "print(f\"Weighted mean: {weighted_mean:0.3f} ± {weighted_error:0.3f}\")\n",
    "print(f\"Truth        : {H0_mean:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f59239",
   "metadata": {},
   "source": [
    "###  Now let's add those to our plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2fe118",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(means, np.arange(len(means)), xerr=(errors), fmt=\".\", color='black')\n",
    "plt.xlim(67, 76)\n",
    "plt.xlabel(\"Mean of sub-sample\")\n",
    "plt.ylabel(\"Experiment number\")\n",
    "plt.errorbar(overall_mean, 2.6, xerr=overall_error, yerr=2.5, fmt='o', color='green', label=\"Full Sample\")\n",
    "plt.errorbar(straight_mean, 2.2, xerr=straight_error, yerr=2.5, fmt='o', color='orange', label=\"Straight Mean\")\n",
    "plt.errorbar(weighted_mean, 2.4, xerr=weighted_error, yerr=2.5, fmt='o', color='blue', label=\"Weighted Mean\")\n",
    "plt.scatter(H0_mean, 2.5, marker='o', color='cyan', label=\"True\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43d936",
   "metadata": {},
   "source": [
    "### Questions for discussion:\n",
    "\n",
    "#### 3.1 In your own words, describe what is being presented in the previous plot and the two cells before that.\n",
    "\n",
    "#### 3.2 In this case, the different methods of combining the results give quite similar, but not identical, final answers.   Do you think that the results are \"consistent enough\"?  I.e., do they seem to agree within their reported errors?\n",
    "\n",
    "#### 3.3 The usual convention for combining results is to use variance weighting (i.e., the weighted mean as computed here).  Does that seem like a sensible convention to you?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7950a0a",
   "metadata": {},
   "source": [
    "### Now let's try the using the uncertainties to get a weighted average of the Hubble measurements\n",
    "\n",
    "Now we can go back to the real Hubble data, and combine the measurements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0_errors = H0_errorLow + H0_errorHigh\n",
    "H0_weights = 1./(H0_errors*H0_errors)\n",
    "\n",
    "H0_wmean  = np.sum(H0_measured*H0_weights)/np.sum(H0_weights)\n",
    "H0_werror = np.sqrt(1/np.sum(H0_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(H0_measured, np.arange(N_measurements), xerr=(H0_errorLow, H0_errorHigh), fmt=\".\", color='k')\n",
    "plt.xlabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "plt.ylabel(\"Experiment number\")\n",
    "plt.errorbar(H0_mean, 18, xerr=H0_err, yerr=20, fmt='o', color='b', label=\"Straight Mean\")\n",
    "plt.errorbar(H0_wmean, 18, xerr=H0_werror, yerr=20, fmt='o', color=\"r\", label=\"Weighted Mean\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fafc9ed",
   "metadata": {},
   "source": [
    "### Questions for discussion:\n",
    "\n",
    "#### 4.1 In this case the weighted mean doesn't quite agree with the straight mean.  Looking at the plot, why do you think that is?\n",
    "\n",
    "#### 4.2 Does this change what you think about what the best estimate of the Hubble parameter might be?  Why?  What about the uncertainty on the Hubble parameter?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

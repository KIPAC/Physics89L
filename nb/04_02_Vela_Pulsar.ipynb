{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f43838",
   "metadata": {},
   "source": [
    "# Gaussian distributions: p-values and trials factors\n",
    "\n",
    "### Goals:\n",
    "\n",
    "1. To take a real scientific example of a Gaussian distribution: the weekly fluctuations of the observed Gamma-ray flux from the Vela pulsar.\n",
    "2. To wonder and marvel at the fact that there is such a thing as the Gamma-ray pulsars.\n",
    "3. To use our understanding of Gaussian and p-values to decide if we are seeing \"flaring episodes\" from the Vela pulsar, or just expected statistical fluctuations.\n",
    "4. To compare the example of the Vela pulsar to an unidentified object in the Gamma-ray sky and decide if it had a \"flaring episode\".\n",
    "\n",
    "### Timing\n",
    "\n",
    "1. Try to finish this notebook in 30-35 minutes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import datetime\n",
    "\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc63c51",
   "metadata": {},
   "source": [
    "### New functions we will use in this module\n",
    "\n",
    "| Function Name            | What it does |\n",
    "| - | - |\n",
    "|    datetime.datetime.fromtimestamp  | Converts time from seconds since a reference to a format with year, month, day, etc..., which matplotlib uses to make nice plot axis labels |\n",
    "|    np.argmax             | Find the index of the largest element in an array  |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7826019",
   "metadata": {},
   "source": [
    "# Fermi Gamma-ray Space Telescope\n",
    "\n",
    "The Fermi Gamma-ray Space Telescope is a NASA mission that was launched in 2008 and has spent the last 13 years observing the sky in gamma-rays.  You can read more about the Fermi mission on the NASA website:  https://fermi.gsfc.nasa.gov/\n",
    "\n",
    "This is a map of the whole sky as seen by Fermi.  The map is in \"Galactic Coordinates\", looking out from the Earth.  The plane of the Galaxy (i.e., the Milky Way) runs horizontally across the map.  The center of the Galaxy (which lies in the constellation of Sagittarius) is at the center of the map.  As you can see from the bright band across the middle of the image, the Galaxy glows very brightly in Gamma Rays. \n",
    "\n",
    "If you look carefully at the map, you might notice that the point sources have some spatial extent to them. In all but a very few cases, that is because of the imperfect resolution of the instrument, not the intrinsic extension of the source. \n",
    "\n",
    "![Fermi sky map](figures/intens_ait_144m_gt1000_psf3_gal_0p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab798fb1",
   "metadata": {},
   "source": [
    "# The Vela Pulsar\n",
    "\n",
    "There is a bright spot close to the Galactic Plane, about halfway between the center of the image and the right edge.  That is the Vela Pulsar.  Although it is hard to see in this image, where the color scale has been adjusted to highlight all the dimmer sources in the sky, the Vela Pulsar is by far the brightest object in the Gamma-ray sky.\n",
    "\n",
    "The Vela pulsar is a neutron star left over from a supernova that occurred about 11 to 12 thousand years ago.  You can read about it here.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Vela_Pulsar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13aabe",
   "metadata": {},
   "source": [
    "# Time Analysis: Light curves \n",
    "\n",
    "One important part of the Fermi scientific mission is to monitor objects in the sky for changes.  It turns out that many of the objects that produce gamma-rays are extremely variable in brightness.  They can flare up to hundreds of times their average brightness in a matter of hours or days.  \n",
    "\n",
    "By design, the Fermi mission tries to observe each part of the sky the same amount of time each week.  For a variety of reasons it doesn't always work out that way.  So, to search for flaring episodes, they use a method where they compare the number of gamma-rays observed from a given source each week to a model that tells them how many gamma-rays to expect from that source.\n",
    "\n",
    "We will be using some of these data, which were obtained from the publically available variability monitoring data here:\n",
    "\n",
    "https://fermi.gsfc.nasa.gov/ssc/data/access/lat/FAVA/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Vela_Flux.txt\", 'rb'), usecols=range(7))\n",
    "\n",
    "## This is how we pull out the data from columns in the array.\n",
    "\n",
    "## This is the date in \"Mission Elapsed Time\" For the Fermi mission, this \n",
    "## is defined to be the number of seconds since the start of 2001.\n",
    "date_MET = data[:,0]\n",
    "\n",
    "## This is the offset in seconds between the Fermi \"MET\" and the UNIX \n",
    "## \"epoch\" used by matplotlib\n",
    "MET_To_Unix = 978336000\n",
    "\n",
    "## These are the number of photons observed from Vela each week in \n",
    "## the \"low\" Energy Band (100 MeV - 800 MeV)\n",
    "nObs_LE = data[:,1]\n",
    "\n",
    "## These are the number of photons expected from Vela each week, under \n",
    "## the assumption that it is not varying at all, and the only differences \n",
    "## depend on how long we spent looking at Vela that particular weeek\n",
    "nExp_LE = data[:,2]\n",
    "\n",
    "## These are the band bounds, in MeV\n",
    "LE_bounds = (100., 800.)\n",
    "\n",
    "## You could also take a look at data in the \"high\" energy band \n",
    "## (800 MeV - 10000 MeV), although that's not part of the lab this week.\n",
    "#nObs_HE = data[:,4]\n",
    "#nExp_HE = data[:,5]\n",
    "#signif_HE = data[:6]\n",
    "#HE_bounds = (800., 10000.)\n",
    "\n",
    "# This converts the dates to something that matplotlib understands\n",
    "dates = [datetime.datetime.fromtimestamp(date + MET_To_Unix) for date in date_MET]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b8efc",
   "metadata": {},
   "source": [
    "Ok, lets plot the number of observed and expected gamma-rays seen from Vela each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate a figure and axis object for more control\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(dates, nExp_LE, label=\"expected\")\n",
    "ax.scatter(dates, nObs_LE, label=\"observed\")\n",
    "ax.set_ylabel(r\"$n$ [per week]\")\n",
    "ax.set_xlabel(\"Date [year]\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372298e8",
   "metadata": {},
   "source": [
    "It looks like the observed data points are pretty close to the expected data points, let's check that by making a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate a figure and axis object for more control\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(nExp_LE, nObs_LE)\n",
    "ax.set_xlabel(r\"$n_{\\rm exp}$ [per week]\")\n",
    "ax.set_ylabel(r\"$n_{\\rm obs}$ [per week]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f6fb6",
   "metadata": {},
   "source": [
    "Yep, they seem highly correlated.  Now let's look at the difference between the observed and expected events and see if anything sticks out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate a figure and axis object for more control\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(dates, nObs_LE-nExp_LE)\n",
    "ax.set_ylabel(r\"$n_{\\rm excess}$ [per week]\")\n",
    "ax.set_xlabel(\"Date [year]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739ca4c",
   "metadata": {},
   "source": [
    "Hmm. Looks pretty random, but it is kind of hard to tell because we don't really know what kind of scatter to expect.  That point up around 200 might be interesting.\n",
    "\n",
    "Now, let's recall that the number of events observed is a random process that can be described by a Gaussian where the width is $\\sigma = \\sqrt{n}$, when the rate is large.  \n",
    "\n",
    "So we are going to divide the excess for each week by the expected statistical fluctation for that week to get a sense of how significant the outliers might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_nObs_LE = np.sqrt(nObs_LE)\n",
    "scaled_residuals = (nObs_LE-nExp_LE)/sigma_nObs_LE\n",
    "\n",
    "## Instantiate a figure and axis object for more control\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(dates, scaled_residuals)\n",
    "ax.set_ylabel(r\"Scaled excess [$\\sigma$]\")\n",
    "ax.set_xlabel(\"Date [year]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f88851",
   "metadata": {},
   "source": [
    "And let's make a histogram of the y-axis values and see if it really looks like a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c99deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate some Gaussian values to compare to\n",
    "nWeeks = len(nObs_LE)\n",
    "myGauss = nWeeks*0.2*stats.norm(loc=0, scale=1).pdf(np.linspace(-4,4,401))\n",
    "\n",
    "## Instantiate a figure and axis object for more control\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.hist(scaled_residuals, bins=np.linspace(-4,4,41))\n",
    "ax.plot(np.linspace(-4,4,401), myGauss)\n",
    "ax.set_xlabel(r\"Scaled excess [$\\sigma$]\")\n",
    "ax.set_ylabel(r\"Entries [per $0.2 \\sigma$]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dcf664",
   "metadata": {},
   "source": [
    "### Interpretation in terms of p-values\n",
    "\n",
    "Ok, those data look pretty Gaussian.  Let's take a look at the largest outliers and see how significant they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22095f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = np.max(scaled_residuals)\n",
    "p_value = stats.norm(loc=0, scale=1).sf(max_value)\n",
    "print(f\"The largest scaled excess is {max_value:0.2f} sigma\")\n",
    "print(f\"The corresponding p-value is {p_value:0.5f}\")\n",
    "print(f\"That corresponds to a 1 in {(1/p_value):0.0f} chance of occuring randomly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd1c0d",
   "metadata": {},
   "source": [
    "### Questions for discussion.\n",
    "\n",
    "#### 4.1 A 1 in 334 chance of occuring naturally seems really unlikely.  However the distribution of scaled residual looks pretty Gaussian, and the scatter plot above looks pretty random;  what is going on? Why does it make sense that the largest scaled residual is about 2.75 sigma even if the data are distributed randomly?  Why wouldn't we consider this to be a really flaring episode?  (Use numbers to back up your explanations.)\n",
    "\n",
    "#### 4.2 Given the way that analysis was done, and that we have observed for 651 weeks, what level of statistical significance might we want to use as a threshold for a flaring episode?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ccea4",
   "metadata": {},
   "source": [
    "## Unidentifed flaring object.\n",
    "\n",
    "Now let's take the data for a different object.  In fact, I picked a very faint object for which, when we average the data out over 12 years, it is hard to even tell if there is anything there or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48676f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unid_data = np.loadtxt(open(\"../data/UNID_Flux.txt\", 'rb'), usecols=range(7))\n",
    "unid_date_MET = unid_data[:,0]\n",
    "unid_nObs_LE = unid_data[:,1]\n",
    "unid_nExp_LE = unid_data[:,2]\n",
    "unid_signif_LE = unid_data[:,3]\n",
    "unid_dates = [datetime.datetime.fromtimestamp(date + MET_To_Unix) for date in unid_date_MET]\n",
    "unid_sigma_nObs_LE = np.sqrt(unid_nObs_LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653f032",
   "metadata": {},
   "source": [
    "#### Excess counts with error bars.\n",
    "\n",
    "First, let's make a slightly different version of the plots we made above.  \n",
    "This time we will plot the excess counts, and use the $\\sqrt{n}$ as the error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate a figure and axis object for more control\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.errorbar(unid_dates, unid_nObs_LE-unid_nExp_LE, yerr=unid_sigma_nObs_LE, fmt='.')\n",
    "ax.set_ylim(-100, 100)\n",
    "ax.set_ylabel(\"Excess counts [per week]\")\n",
    "ax.set_xlabel(\"Date [year]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd9e1be",
   "metadata": {},
   "source": [
    "### Interpretation in terms of p-values:\n",
    "\n",
    "A few of those points look pretty significant.  Let's quantify them in terms of their p-values.\n",
    "\n",
    "First, let's plot the scaled excess. We'll take this quantity directly from the Fermi data, which does an analysis which is a bit more complex than simply dividing the excess counts by $\\sqrt{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce848d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate a figure and axis object for more control\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(unid_dates, unid_signif_LE)\n",
    "ax.set_ylabel(r\"Scaled excess [$\\sigma$]\")\n",
    "ax.set_xlabel(\"Date [year]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4272dae1",
   "metadata": {},
   "source": [
    "Now let's make a histogram of the scaled excess and see if it looks Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "myGauss = len(unid_signif_LE)*0.5*stats.norm(loc=0, scale=1).pdf(np.linspace(-8,8,241))\n",
    "\n",
    "## Instantiate a figure and axis object for more control\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.hist(unid_signif_LE, bins=np.linspace(-8, 8, 33))\n",
    "ax.plot(np.linspace(-8,8,241), myGauss)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ae783",
   "metadata": {},
   "source": [
    "Well, it looks pretty Gaussian, but there are definitely a few outliers at the > 4 sigma level, and maybe a slight excess of weeks at the 2-3 sigma level.  Let's follow up on the largest outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value_unid = np.max(unid_signif_LE)\n",
    "p_value_unid = stats.norm(loc=0, scale=1).sf(max_value_unid)\n",
    "\n",
    "print(f\"The largest scaled excess is {max_value_unid:0.2f} sigma\")\n",
    "print(f\"The corresponding p-value is {p_value_unid:0.2e}\")\n",
    "print(f\"That corresponds to a 1 in {(1/p_value_unid):0.0e} chance of occuring randomly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db284cc2",
   "metadata": {},
   "source": [
    "Ok, it seems pretty significant, let's print some information about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_index = np.argmax(unid_signif_LE)\n",
    "flare_date = unid_dates[flare_index]\n",
    "\n",
    "print(f\"Flare detected in the week of {flare_date.strftime('%Y-%m-%d')} at the level of {max_value_unid:0.1f} sigma significance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2010859",
   "metadata": {},
   "source": [
    "### Questions for discussion\n",
    "\n",
    "#### 5.1 In contrast to the case for the Vela pulsar, here the chance for an outlier this size occurring randomly is tiny, even given the fact that we are considering over 600 weeks of data.  In fact we'd have to observe for something like $2 \\times 10^{11}$ = 200 billion weeks to expect a random fluctuation of this size.  On the other hand, the change in significance only went from $2.75 \\sigma$ to $6.81 \\sigma$. Explain, in your own words, what this means in terms of how we should think about significance in terms of $\\sigma$ as compared to significance in terms of p-values.  \n",
    "\n",
    "#### 5.2 Does this change your opinions about the conventions used to define statistical significance?  If someone reports a result with a p-value of $p < 0.05$ what are some follow up questions you might ask them?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

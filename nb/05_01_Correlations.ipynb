{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef323bcc",
   "metadata": {},
   "source": [
    "# Introduction to Correlations and Covariance\n",
    "\n",
    "### Goals:\n",
    "\n",
    "1. To develop intuition for correlation with visual examples.\n",
    "2. To understand how a \"covariance matrix\" encodes infomation about correlations.\n",
    "3. To understand the difference between \"covariance\" and \"correlation\" and the advantages and disadvantages of each.\n",
    "4. To deepen our understanding with examples of quantities that are highly correlated and quantities that are uncorrelated.\n",
    "\n",
    "### Timing\n",
    "\n",
    "1. Try to finish this notebook in 25-30 minutes. \n",
    "\n",
    "### Question and Answer Template\n",
    "\n",
    "You can go to the link below, and do \"file\" -> \"make a copy\" to make yourself a google doc that you can use to fill in the answers to the question in this weeks notebooks.\n",
    "\n",
    "https://docs.google.com/document/d/1D3O8AjzSNhCOFeKVrSXmRWSKdOFIdSMkdNTPZ-CIdaM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfed2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64debd36",
   "metadata": {},
   "source": [
    "### New functions we will use in this module\n",
    "\n",
    "| Function Name            | What it does |\n",
    "| - | - |\n",
    "| plt.figure               | Make a matplotlib figure, useful for making figures with subplots |\n",
    "| fig.subplots             | Makes subplots for a figure |\n",
    "| np.cov                   | Compute the covariance matrix between multiple data series |\n",
    "| np.corrcoef              | Compute the correlation coefficient between multiple data series |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0788d",
   "metadata": {},
   "source": [
    "### Variances and covariances:\n",
    "\n",
    "The **variance** $\\sigma^2$ is a measure of the scatter of a quantity:\n",
    "\n",
    "$\\sigma^2 = \\frac{\\sum_i (x_i - \\mu_x)}{n}^2$,\n",
    "\n",
    "where $x_i$ are the measurements of the quantity $x$, $n$ is the number of measurements, and $\\mu_x$ is the mean of the measurements $\\mu_x = \\frac{\\sum_i x_i}{n}$.\n",
    "\n",
    "The **covariance** is a measure of how variations in one quantity match variations in a second quantity. \n",
    "\n",
    "The equation for the covariance is quite similar to the equation for the variance. Suppose that each data point has has two quantities that describe it, $(x_{i},y_{i})$. (In other words, it's a data point in a multi-dimensional space.) Then the equation for the covariance is:\n",
    "\n",
    "$\\sigma_{xy} = \\frac{\\sum_i (x_i - \\mu_x) (y_i - \\mu_y)}{n}$\n",
    "\n",
    "i.e., we replace one of the factors of $(x_i - \\mu_x)$ with $(y_i - \\mu_y)$.\n",
    "\n",
    "A couple of things to notice about the covariance:\n",
    "\n",
    "1. If, for a given pair of values, both $(x_i - \\mu_x)$ and $(y_i - \\mu_y)$ have the same sign, (i.e., they are both above or below the average), then $(x_i - \\mu_x) (y_i - \\mu_y)$ will be greater than zero.  Conversely, if they have opposite signs, then $(x_i - \\mu_x) (y_i - \\mu_y)$ will be less than zero.   Thus, the covariance can be either positive or negative.\n",
    "\n",
    "2. Pairs of values where the absolute values $|(x_i - \\mu_x)|$ and $|(y_i - \\mu_y)|$ are both large will contribute a lot to the covariance.  Pairs where one of the values is very close to the mean will contribute very little.\n",
    "\n",
    "Because the equations for the variance and covariance are so similar, we often compute both the variances and covariances at the same time and put all the results in an array or matrix. This is called the **covariance matrix**:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{xx} & \\sigma_{xy} \\\\\n",
    "\\sigma_{yx} & \\sigma_{yy} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e48db",
   "metadata": {},
   "source": [
    "### Let's make a couple of functions to simulate 2D data with different levels of covariances\n",
    "\n",
    "The `draw_2d_gaussian` function below draws data from a **pair** of Gaussians centered at zero ($\\mu_x = \\mu_y = 0$).\n",
    "\n",
    "We need to specify:\n",
    "\n",
    "- \"axes\" → plotting option\n",
    "\n",
    "- \"color\" → plotting option\n",
    "\n",
    "- $n$ → number of data points\n",
    "\n",
    "- $\\sigma_{xx}$ → variance of Gaussian for variable x\n",
    "\n",
    "- $\\sigma_{yy}$ → variance of Gaussian for variable y\n",
    "\n",
    "- $\\sigma_{xy}$ → covariance of Gaussian for variables x and y\n",
    "\n",
    "You can safely ignore the details of the functions, but if you're curious, [here](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Drawing_values_from_the_distribution) is an explanation of the method of how to specify a multi-dimentional normal distribution with a covariance matrix.  The point here is to visualize the resulting scatter plots and measure the covariances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0474db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_2d_gaussian(n, sigma_xx, sigma_yy, sigma_xy):\n",
    "    K_0 = np.array([[sigma_xx, sigma_xy],[sigma_xy, sigma_yy]]) # make a matrix-like object \n",
    "    epsilon = 0.0001\n",
    "    K = K_0 + epsilon*np.identity(2)\n",
    "    L = np.linalg.cholesky(K)\n",
    "    u = np.random.normal(size=2*n).reshape(2, n)\n",
    "    x = np.dot(L, u)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_2d_gaussian(axes, color, n, sigma_xx, sigma_yy, sigma_xy):\n",
    "    vals = gen_2d_gaussian(n, sigma_xx, sigma_yy, sigma_xy)\n",
    "    axes.set_xlim(-5, 5)\n",
    "    axes.set_ylim(-5, 5)\n",
    "    axes.scatter(vals[0], vals[1], color=color)\n",
    "    axes.set_xlabel(\"x\")\n",
    "    axes.set_ylabel(\"y\")\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1059a",
   "metadata": {},
   "source": [
    "### Effect of changing the covariance, $\\sigma_{xy}$.\n",
    "\n",
    "Let's make 4 plots with unit variances and with 4 different values of the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe1a12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.subplots(2,2)\n",
    "vals_0 = draw_2d_gaussian(axes[0,0], 'r', 1000, 1, 1, 0)\n",
    "vals_1 = draw_2d_gaussian(axes[0,1], 'g', 1000, 1, 1, 1)\n",
    "vals_2 = draw_2d_gaussian(axes[1,0], 'b', 1000, 1, 1, -1)\n",
    "vals_3 = draw_2d_gaussian(axes[1,1], 'y', 1000, 1, 1, 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance matrix from the generated data\n",
    "print(\"Covariance matrix for plot 0,0:\\n\", np.cov(vals_0))\n",
    "print(\"Covariance matrix for plot 0,1:\\n\", np.cov(vals_1))\n",
    "print(\"Covariance matrix for plot 1,0:\\n\", np.cov(vals_2))\n",
    "print(\"Covariance matrix for plot 1,1:\\n\", np.cov(vals_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6e22f",
   "metadata": {},
   "source": [
    "### Effect of changing the variances, $\\sigma_{xx}, \\sigma_{yy}$:\n",
    "\n",
    "Let's make 4 plots with different variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54612e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.subplots(2,2)\n",
    "vals_0 = draw_2d_gaussian(axes[0,0], 'r', 1000, 2, 2, 0)\n",
    "vals_1 = draw_2d_gaussian(axes[0,1], 'g', 1000, 0.2, 1, 0)\n",
    "vals_2 = draw_2d_gaussian(axes[1,0], 'b', 1000, 2, 0.2, 0)\n",
    "vals_3 = draw_2d_gaussian(axes[1,1], 'y', 1000, 0.2, 0.2, 0.18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute covariance matrix from the generated data\n",
    "print(\"Covariance matrix for plot 0,0:\\n\", np.cov(vals_0))\n",
    "print(\"Covariance matrix for plot 0,1:\\n\", np.cov(vals_1))\n",
    "print(\"Covariance matrix for plot 1,0:\\n\", np.cov(vals_2))\n",
    "print(\"Covariance matrix for plot 1,1:\\n\", np.cov(vals_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fbaa2",
   "metadata": {},
   "source": [
    "### Questions for discussion\n",
    "\n",
    "#### 1.1 Do the plots look how you expected? Explain how the shapes and spread of the four plots relate to the values you see in the covariance matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da69169",
   "metadata": {},
   "source": [
    "### Another way to consider correlations is to ask what part of the variance in one quantity is  tied to the variance of another quantity.\n",
    "\n",
    "To do this, we want to factor out the variances of the two quantities and compute the \"correlation factor\" or \"correlation coefficient\".\n",
    "\n",
    "$c_{xy} = \\frac{\\sigma_{xy}}{\\sqrt{\\sigma_{xx}\\sigma_{yy}}}$\n",
    "\n",
    "Note that $c_{xx} = c_{yy} = 1$, i.e., each quantity is 100% correlated with itself.\n",
    "\n",
    "We can again put together $c_{xx}$, $c_{yy}$, $c_{xy}$ in a matrix called the **correlation matrix**:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "c_{xx} & c_{xy} \\\\\n",
    "c_{yx} & c_{yy} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's remake the second set of plots and evaluate the correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.subplots(2,2)\n",
    "vals_0 = draw_2d_gaussian(axes[0,0], 'r', 1000, 2, 2, 0)\n",
    "vals_1 = draw_2d_gaussian(axes[0,1], 'g', 1000, 0.2, 1, 0)\n",
    "vals_2 = draw_2d_gaussian(axes[1,0], 'b', 1000, 2, 0.2, 0)\n",
    "vals_3 = draw_2d_gaussian(axes[1,1], 'y', 1000, 0.2, 0.2, 0.18)\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation matrix for plot 0,0:\\n\", np.corrcoef(vals_0))\n",
    "print(\"Correlation matrix for plot 0,1:\\n\", np.corrcoef(vals_1))\n",
    "print(\"Correlation matrix for plot 1,0:\\n\", np.corrcoef(vals_2))\n",
    "print(\"Correlation matrix for plot 1,1:\\n\", np.corrcoef(vals_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee2d8b",
   "metadata": {},
   "source": [
    "### Question for discussion\n",
    "\n",
    "#### 2.1 What do you see as the advantage / disadvantage of using the correlation coefficient versus the covariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb8cdd",
   "metadata": {},
   "source": [
    "## Ok, let's load up the Vela data\n",
    "\n",
    "This is the same stuff we used to load the data last week.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Vela_Flux.txt\", 'rb'), usecols=range(7))\n",
    "\n",
    "# This is how we pull out the data from columns in the array.\n",
    "\n",
    "# This is the date in \"Mission Elapsed Time\"\n",
    "# For the Fermi mission, this is defined to be the number of seconds since the start of 2001.\n",
    "date_MET = data[:,0]\n",
    "# This is the offset in seconds between the Fermi \"MET\" and the UNIX \"epoch\" used by matplotlib\n",
    "MET_To_Unix = 978336000\n",
    "\n",
    "# These are the numbers of photons observed from Vela each week in the \"low\" Energy Band (100 MeV - 800 MeV)\n",
    "nObs_LE = data[:,1]\n",
    "\n",
    "# These are the number of photons expected from Vela each week, under the assumption that it is \n",
    "# not varying at all, and the only differences depend on how long we spent looking at Vela\n",
    "# that particular weeek\n",
    "nExp_LE = data[:,2]\n",
    "\n",
    "# These are the band bounds, in MeV\n",
    "LE_bounds = (100., 800.)\n",
    "\n",
    "# This is the \"significance\" of the variation for each week.  We will discuss this more later\n",
    "signif_LE = data[:,3]\n",
    "\n",
    "nObs_HE = data[:,4]\n",
    "nExp_HE = data[:,5]\n",
    "signif_HE = data[:6]\n",
    "HE_bounds = (800., 10000.)\n",
    "\n",
    "# This converts the dates to something that matplotlib understands\n",
    "dates = [datetime.datetime.fromtimestamp(date + MET_To_Unix) for date in date_MET]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f42fc7",
   "metadata": {},
   "source": [
    "### Example with quantities that are highly correlated.\n",
    "\n",
    "Last week we saw that for the Vela data the numbers of observed and expected counts where highly correlated.  I.e., that Vela wasn't flaring unexpectedly.\n",
    "\n",
    "Let's quantify that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(nExp_LE, nObs_LE)\n",
    "plt.xlabel(r\"$n_{\\rm exp}$ [per week]\")\n",
    "plt.ylabel(r\"$n_{\\rm obs}$ [per week]\")\n",
    "plt.show()\n",
    "\n",
    "correl = np.corrcoef(nObs_LE, nExp_LE)\n",
    "print(f\"The expected counts are {correl[0,1]:0.3f} correlated with the observed counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7efb3",
   "metadata": {},
   "source": [
    "### Is there a long-term trend to the Vela flux?\n",
    "\n",
    "Let's use the correlation to see if there is a long-term trend to the Vela pulsar flux. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ef205",
   "metadata": {},
   "source": [
    "Ok, so let's convert the time axis to a nice small number (i.e., let's scale the time to years, and let's set the zero point to something in the middle of the time range).  This will save us having lots of really big numbers floating around, which would happen if we left the time in seconds since 2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_YEAR = 2001 +  (date_MET / (24*3600*365))\n",
    "years_since_mid_2014 = date_YEAR  - 2014.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd0065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "excess_counts = nObs_LE-nExp_LE\n",
    "plt.scatter(years_since_mid_2014, excess_counts)\n",
    "plt.xlabel(r\"Time since mid 2014 [years]\")\n",
    "plt.ylabel(r\"$n_{\\rm obs}$ [per week]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(years_since_mid_2014, excess_counts)\n",
    "correl = np.corrcoef(years_since_mid_2014, excess_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5f016",
   "metadata": {},
   "source": [
    "### Let's have a look at the pieces of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491359b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"The xx element of the covariance matrix is {cov[0,0]:.2f} years**2\")\n",
    "print(f\"The standard deviation of the x element (i.e., the year) is {np.sqrt(cov[0,0]):.2f} years\")\n",
    "print(f\"The yy element of the covariance matrix is {cov[1,1]:.2f} counts**2\")\n",
    "print(f\"The standard deviation of the yy element (i.e., the excess counts) is {np.sqrt(cov[1,1]):.1f} counts\")\n",
    "print(f\"The xy element of the covariance matrix is {cov[0,1]:.2f} years*counts\")\n",
    "print(f\"The x-y correlation coefficient is {correl[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d02680",
   "metadata": {},
   "source": [
    "### Question for discussion\n",
    "\n",
    "#### 3.1 Do you think that the small correlation we found in time vs $n_{\\rm obs}$ is going to be statistically significant?  Why or why not?  (Some things you might want to think about: Does the contrast between the last two plots affect your opinion?  What about the difference between the correlation values?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

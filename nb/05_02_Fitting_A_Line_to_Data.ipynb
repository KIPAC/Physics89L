{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e332f5",
   "metadata": {},
   "source": [
    "# Introduction to model fitting\n",
    "\n",
    "### Goals:\n",
    "\n",
    "1. To understand what it means to fit a model, and to derive estimates for the values on the parameters in the model.\n",
    "2. To fit a linear model for the time variation of the flux of the Vela pulsar to the data.\n",
    "3. To use the results of that fit to decide if the flux of the Vela pulsar is, in fact, varying with time.\n",
    "\n",
    "### Timing\n",
    "\n",
    "1. Try to finish this notebook in 35-45 minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "import datetime\n",
    "\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531c449",
   "metadata": {},
   "source": [
    "### New functions we will use in this module\n",
    "\n",
    "| Function Name            | What it does |\n",
    "| - | - |\n",
    "|  plt.contour |  Make a contour plot, ie., show the contours correspond to a series of values |\n",
    "| scipy.stats.chi2 | Interact with a $\\chi^2$ distribution, e.g., to compute a p-value |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ede43d",
   "metadata": {},
   "source": [
    "### This figure summarizes the essence of this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f551d1",
   "metadata": {},
   "source": [
    "<img src=\"figures/Fitting.png\" alt=\"drawing\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df2c8f",
   "metadata": {},
   "source": [
    "### Ok, let's pick up where we left off with the Vela pulsar data!\n",
    "\n",
    "(This cell is just a repeat of loading the Vela data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b70a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Vela_Flux.txt\", 'rb'), usecols=range(7))\n",
    "\n",
    "## This is how we pull out the data from columns in the array.\n",
    "\n",
    "## This is the date in \"Mission Elapsed Time\" For the Fermi mission, this \n",
    "## is defined to be the number of seconds since the start of 2001.\n",
    "date_MET = data[:,0]\n",
    "\n",
    "## This is the offset in seconds between the Fermi \"MET\" and the UNIX \n",
    "## \"epoch\" used by matplotlib\n",
    "MET_To_Unix = 978336000\n",
    "\n",
    "## These are the number of photons observed from Vela each week in \n",
    "## the \"low\" Energy Band (100 MeV - 800 MeV)\n",
    "nObs_LE = data[:,1]\n",
    "\n",
    "## These are the number of photons expected from Vela each week, under \n",
    "## the assumption that it is not varying at all, and the only differences \n",
    "## depend on how long we spent looking at Vela that particular weeek\n",
    "nExp_LE = data[:,2]\n",
    "\n",
    "## These are the band bounds, in MeV\n",
    "LE_bounds = (100., 800.)\n",
    "\n",
    "## We will also take a look at data in the \"high\" energy band \n",
    "## (800 MeV - 10000 MeV)\n",
    "nObs_HE = data[:,4]\n",
    "nExp_HE = data[:,5]\n",
    "signif_HE = data[:6]\n",
    "HE_bounds = (800., 10000.)\n",
    "\n",
    "## This converts the dates to something that matplotlib understands\n",
    "dates = [datetime.datetime.fromtimestamp(date + MET_To_Unix) for date in date_MET]\n",
    "\n",
    "## Convert the dates to years to make the numbers more sensible\n",
    "date_YEAR = 2001 +  (date_MET / (24*3600*365))\n",
    "years_since_mid_2014 = date_YEAR  - 2014.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0a1cc",
   "metadata": {},
   "source": [
    "So, we are going to investigate if the small correlation between the excess and the time is statistically significant.   Let's start by looking at the data and adding uncertainties to the points and plotting it using the variables we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the excess counts and estimate an uncertainty on the excess counts\n",
    "## given by the square root of the observed counts, which assuming Poisson statistics\n",
    "excess_counts = nObs_LE-nExp_LE\n",
    "sigma_counts =  np.sqrt(nObs_LE)\n",
    "\n",
    "## Plot the excess counts as a function of time, with some uncertainties\n",
    "## and some custom plotting options to improve clarity\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.errorbar(years_since_mid_2014, excess_counts, \n",
    "            fmt='ko', ms=4, ecolor='pink', yerr=sigma_counts)\n",
    "\n",
    "ax.set_xlabel(r\"Time since mid 2014 [years]\")\n",
    "ax.set_ylabel(r\"$n_{\\rm excess}$ [per week]\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d696e5",
   "metadata": {},
   "source": [
    "# Step 1.   The model\n",
    "\n",
    "We just write a simple function for our model, and plot how the model looks for a few values of the parameters. Call $p_0$ and $p_1$ our parameters, representing the offset and slope in a linear model, respectively. Then our model for time dependence, given these parameters, is:\n",
    "\n",
    "$n_{\\rm excess}(t | p_0, p_1) = p_0 + t * p_1$ \n",
    "\n",
    "In the next cell we will define a function that returns the model prediction for the y-values (i.e., excess counts) if you pass in a set of x-values (i.e., times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b798ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(xvals, params):\n",
    "    return params[0] + xvals*params[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473732d",
   "metadata": {},
   "source": [
    "Now we are going to plot that model for a few different sets of paramters.\n",
    "\n",
    "First we will change $p_0$, the offset parameter, and plot that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start by making the same plot as before with our excess counts and \n",
    "## associated uncertainties\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.errorbar(years_since_mid_2014, excess_counts, \n",
    "            fmt='ko', ms=4, ecolor='pink', yerr=sigma_counts)\n",
    "\n",
    "ax.set_xlabel(r\"Time since mid 2014 [years]\")\n",
    "ax.set_ylabel(r\"$n_{\\rm excess}$ [per week]\")\n",
    "\n",
    "## Plot the linear model for a few different values of the constant offset\n",
    "## i.e. parameter p0, i.e. params[0], all with a slope of zero\n",
    "xvals = years_since_mid_2014\n",
    "params = np.array([0, 0])\n",
    "for offset in np.linspace(-200, 200, 5):\n",
    "    params[0] = offset\n",
    "    ax.plot(xvals, linear_function(xvals, params), lw=2, \n",
    "            label=rf\"Offset = {offset:0.0f}\", zorder=5)\n",
    "\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f925401",
   "metadata": {},
   "source": [
    "Now we will change $p_1$, the slope, and plot that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start by making the same plot as before with our excess counts and \n",
    "## associated uncertainties\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.errorbar(years_since_mid_2014, excess_counts, \n",
    "            fmt='ko', ms=4, ecolor='pink', yerr=sigma_counts)\n",
    "\n",
    "ax.set_xlabel(r\"Time since mid 2014 [years]\")\n",
    "ax.set_ylabel(r\"$n_{\\rm excess}$ [per week]\")\n",
    "\n",
    "## Plot the linear model for a few different values of the slope parameter\n",
    "## i.e. parameter p1, i.e. params[1], all with a constant offset of 0\n",
    "xvals = years_since_mid_2014\n",
    "params = np.array([0, 0])\n",
    "for slope in np.linspace(-15, 15, 5):\n",
    "    params[1] = slope\n",
    "    ax.plot(xvals, linear_function(xvals, params), lw=2,\n",
    "            label=rf\"Slope = {slope:0.1f}\", zorder=5)\n",
    "\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af226cc4",
   "metadata": {},
   "source": [
    "### An important distinction:\n",
    "\n",
    "When we refer to \"the model\", we are referring to the function which states that the number of counts changes linearly with time. A particular *version* of the model is one with a specific set of parameters.  \n",
    "\n",
    "Keep that in mind as you answer the next question, and for the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbc6d3",
   "metadata": {},
   "source": [
    "### Questions for discussion\n",
    "\n",
    "#### 4.1  How well do you think this model describes the data?  Is this a sensible model?   What would it mean if $p_0$ were very different from 0?  What about if $p_1$ were different from zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a78c78",
   "metadata": {},
   "source": [
    "# Step 2.   The residuals\n",
    "\n",
    "Let's try to quantify the agreement of the model with the data.\n",
    "\n",
    "Let's write a function to plot the **residuals** given the model parameters.  \n",
    "\n",
    "Let's define the residuals as: $\\delta_i = n_{\\rm ex,i} - m_{\\rm ex,i} = n_{\\rm ex,i} - (p_0  + t * p_1)$, or in words, the difference between the model predictions and the measured data points.\n",
    "\n",
    "The first function `residual_function` computes the residuals $\\delta_i$ for each week.  \n",
    "\n",
    "The second function `plot_residuals` makes scatter plots of the residuals for different sets of model parameters versus time.\n",
    "\n",
    "The third function `hist_residuals` makes histograms of the residuals for different sets of model parameters over all the weeks. \n",
    "\n",
    "Let's test these functions on two sets of model parameters: one where $(p_0,p_1) = (0,0)$, and one where $(p_0,p_1) = (0,20)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c255a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function that calculates the residuals between the data and the model,\n",
    "## where the 'model' argument is itself a function of the form: model(xvals, params)\n",
    "def residual_function(data_x, data_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    residual = data_y - model_y\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6adcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to plot residuals for multiple arrays of the dependent\n",
    "## variable, given a single array of the independent variable\n",
    "def plot_residuals(xvals, yvals_list, ylabel=r'$\\delta$ [counts]'):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    ax.set_xlabel(r\"Time since mid 2014 [years]\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    for yvals in yvals_list:\n",
    "        ax.scatter(xvals, yvals)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The same type of function as above, but now for histograms of the residuals,\n",
    "## with an optional argument to specify the binning. The optional argument is\n",
    "## given a default value approriate for a standard normal distribution.\n",
    "def hist_residuals(yvals_list, bins=np.linspace(-6, 6, 61), \\\n",
    "                   xlabel=r'$\\delta$ [counts]', ylabel='# of weeks'):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    for yvals in yvals_list:\n",
    "        ## Use the 'step' type to make a histogram with only the outlines\n",
    "        ## so we can visualize multiple histograms on the same plot\n",
    "        ax.hist(yvals, bins=bins, histtype='step', lw=2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5049279",
   "metadata": {},
   "source": [
    "Now we'll take a look at the residuals for 2 sets of parameters, both with $p_0 = 0$: and one with $p_1 = 0$, and one with $p_1 = 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_0_0  = residual_function(years_since_mid_2014, excess_counts, linear_function, [0,0])\n",
    "resid_0_20 = residual_function(years_since_mid_2014, excess_counts, linear_function, [0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6032bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that the first set of residuals plotted will be blue, and the second set will be orange\n",
    "plot_residuals(years_since_mid_2014, [resid_0_0, resid_0_20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d3fee",
   "metadata": {},
   "source": [
    "### Quick question\n",
    "\n",
    "##### Do the residuals match what you would expect? Remember that we've defined the residuals as: (resid) = (data) - (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot a histogram of the residuals, specifying the binning based on the\n",
    "## range of the residuals observed above.\n",
    "hist_residuals([resid_0_0, resid_0_20], bins=np.linspace(-300, 300, 61))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c2cfe",
   "metadata": {},
   "source": [
    "# Step 3.   The scaled residuals\n",
    "\n",
    "We see there's some scatter to the residuals. However, we also know from previous notebooks that the data itself has an inherent spread, which we want to take into account when trying to assess the agreement between a model and data.\n",
    "\n",
    "Let's write a function to plot the **scaled residuals** (i.e.,  the residuals divided by the uncertainties on the data points to scale them down to a unit Gaussian), given the model parameters:\n",
    "\n",
    "$$\\chi_i =  \\frac{n_{\\rm ex,i} - m_{\\rm ex,i}}{\\sigma_{obs, i}} = \\frac{n_{\\rm ex,i} - (p_0  + t_i * p_1)}{\\sigma_{obs, i}}$$\n",
    "\n",
    "Note that these residuals are now dimensionless! This allows us to make some general assessments about the statistical fluctuations in the data and compare them to normalized distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d8dba",
   "metadata": {},
   "source": [
    "#### Here we scale the individual residuals, so that we can make nice scatter plots of the scaled residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e66bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale each array of residuals by the uncertainty on the counts for that particular observation\n",
    "chi_0_0 = resid_0_0 / sigma_counts\n",
    "chi_0_20 = resid_0_20 / sigma_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot these new scaled residuals.\n",
    "plot_residuals(years_since_mid_2014, [chi_0_0, chi_0_20], \\\n",
    "               ylabel=r'Scaled Excess [$\\sigma$]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91086c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can plot these residuals without specifiying the binning, since the\n",
    "## default binning is appropriate for a standard normal distribution and \n",
    "## we're working with scaled residuals.\n",
    "hist_residuals([chi_0_0, chi_0_20], xlabel=r'Scaled Excess [$\\sigma$]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e42b45",
   "metadata": {},
   "source": [
    "###  Questions for discussion.\n",
    "\n",
    "#### 5.1 Using what we learned last week about Gaussian distributions and p-values, explain why the two plots with the scaled residuals are much more useful than the two plots with the unscaled residuals.\n",
    "\n",
    "#### 5.2 Given the plot above, do you think that there is any chance that the true value of $p_1$ is actually 20 counts / year?  Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bd176",
   "metadata": {},
   "source": [
    "# Step 4.   The $\\chi^2$ distribution\n",
    "\n",
    "Let's formulate a single number to summarize these residuals. Let's define: \n",
    "\n",
    "$$f(\\chi_i) = \\sum_i \\chi_i^2,$$\n",
    "\n",
    "where $\\chi_i$ is the scaled residual for data point $i$.\n",
    "\n",
    "The expected distribution of $f(\\chi_i)$ is known, assuming $\\chi_i$ follows a normal distribution centered at zero with $\\sigma$ = 1. It should follow what is known as a $\\chi^2$ distribution (given by the `scipy.stats.chi2` function). \n",
    "\n",
    "What types of numbers are described by the $\\chi^2$ distribution? \n",
    "\n",
    "If we draw values $X_i$ from a normal distribution centered at zero with $\\sigma$ = 1, then the sum of the squares of $X_i$ will follow a $\\chi^2$ distribution. The form of this distribution has only one parameter, which we call the \"degrees of freedom\" (also called $DOF$ or $d_{f}$ in shorthand). The mean of the $\\chi^2$ distribution is $d_f$, and the variance is $2d_f$.\n",
    "\n",
    "Let's explore the distribution a little bit before going further with the Vela pulsar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850ddbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "## Loop over different values of the degrees of freedom and plot the\n",
    "## associated chi^2 distribution\n",
    "x = np.arange(0,10,.05)\n",
    "for df in [1,2,3,4,5]:\n",
    "    ax.plot(x, stats.chi2(df=df).pdf(x), lw=2, label=rf\"$d_f = {df:d}$\")\n",
    "\n",
    "ax.set_xlabel('$\\chi_{d_f}^2$')\n",
    "ax.set_ylabel('Probability')\n",
    "\n",
    "ax.set_ylim(0,0.5)\n",
    "\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207c2bf",
   "metadata": {},
   "source": [
    "In our case, the $\\chi^2$ distribution applies because we are summing the squares of the scaled residuals.\n",
    "\n",
    "As suggested by the name, \"degrees of freedom\" refers to the number of *independent* pieces of information we use when comparing observations to a model. In our case, this means the number of data points we have collected, minus the number of parameters in our model. So for our quantity $\\chi^2$, the degrees of freedom are:\n",
    "\n",
    "$d_f = n - m,$\n",
    "\n",
    "where $n$ is the number of data points and $m$ is the number of fit parameters (so in our case, 2 for $p_0$ and $p_1$).\n",
    "\n",
    "$f(\\chi_i)$ is usually just referred to as $\\chi^2$, or \"chi-squared\", because it follows the $\\chi^2$ distribution. That's how we'll refer to it from now on. Since the mean of the $\\chi^2$ distribution is $d_f$, for a \"good\" fit, $\\chi^2/d_f$ should be approximately 1. \n",
    "\n",
    "Let's calculate some values of $\\chi^2$ for our data and two sets of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_function(data_x, data_y, data_sigma_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    chi2 = ((data_y - model_y)/(data_sigma_y))**2\n",
    "    return np.sum(chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60186d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_0_0 = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, [0,0])\n",
    "chi2_0_20 = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, [0,20])\n",
    "\n",
    "## Manually compute the value of chi^2 for the first model, which \n",
    "## is the same as the sum of the squares of the scaled residuals. \n",
    "## We do this to check that our function definition is working.\n",
    "chi2_check = np.sum(chi_0_0*chi_0_0)\n",
    "\n",
    "## Compute the degrees of freedom from the size of the data and\n",
    "## the known number of parameters\n",
    "DOF = chi_0_0.size - 2\n",
    "\n",
    "print(f\"    Chi-squared(0,0)  : {chi2_0_0:0.1f}\")\n",
    "print(f\"    Chi-squared check : {chi2_check:0.1f}\")\n",
    "print(f\" Chi-squared(0,0)/DOF : {chi2_0_0/DOF:0.1f}\\n\")\n",
    "print(f\"    Chi-squared(0,20) : {chi2_0_20:0.1f}\")\n",
    "print(f\"Chi-squared(0,20)/DOF : {chi2_0_20/DOF:0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7636c",
   "metadata": {},
   "source": [
    "Using the $\\chi^2$ distribution, we can calculate the likelihood of observing a dataset with the given $\\chi^2$ value if our model parameters are correct.\n",
    "\n",
    "We'll use the same method as last week where we calculate a \"survival fraction\" by integrating the underlying distribution above the observed value of $\\chi^2$, which yields something similar to a $p$-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The odds of seeing the observed data given params (0.,0.) are {stats.chi2(df=DOF).sf(chi2_0_0):.1e}\")\n",
    "print(f\"The odds of seeing the observed data given params (0.,20.) are {stats.chi2(df=DOF).sf(chi2_0_20):.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f66e34b",
   "metadata": {},
   "source": [
    "### Question for discussion:\n",
    "\n",
    "#### 6.1 We just saw that in an idealized case, the chance of seeing the observed data given model parameters (0, 20) is astonishingly less likely (by a factor of $10^{126}$) than the chance of seeing the observed data if the model parameters were (0, 0).  Explain why this is, using information from the plots above question 5.1 and 5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda8cd5",
   "metadata": {},
   "source": [
    "# Step 5.  Minimizing the $\\chi^2$\n",
    "\n",
    "Now that we have a metric to quantify the difference between the model prediction and the data point, we can use this metric to find the values of $p_0,p_1$ which best describe the model. To do this, we can scan over parameter values and try to minimize the $\\chi^2$ value. This effectively minimizes the residuals between the model and the data as a function of the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a8e38",
   "metadata": {},
   "source": [
    "# Scanning parameters.\n",
    "\n",
    "We are first going to find the parameter values that minimize the $\\chi^2$ by scanning each of the parameters and computing the resulting $\\chi^2$. \n",
    "\n",
    "First we are going to scan each variable on its own.  Then we will do a two dimensional scan.\n",
    "\n",
    "We will do $p_0$ (the offset parameter) first.  We will scan from -5 to 5 counts in 11 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a number of different parameter values to scan over\n",
    "nscan = 51\n",
    "\n",
    "## Start with both parameters set to zero\n",
    "params = np.array([0., 0.])\n",
    "\n",
    "## Make a list in which to store the chi^2 values for each parameter set\n",
    "offset_chi2_vals = np.zeros((nscan))\n",
    "\n",
    "## Make an array of offsets to scan over, and loop over each offset\n",
    "offset_scan_points = np.linspace(-5., 5., nscan)\n",
    "for i in range(nscan):\n",
    "    params[0] = offset_scan_points[i]\n",
    "    offset_chi2_vals[i] = chi2_function(years_since_mid_2014, excess_counts, \n",
    "                                        sigma_counts, linear_function, params)\n",
    "\n",
    "## Subtract the minimum value from the computed chi^2 values to get\n",
    "## the \"delta chi^2\" values. We'll assess the minimum value later, \n",
    "## since here we're concerned with how the chi^2 changes as we vary\n",
    "## the offset parameter.\n",
    "offset_chi2_vals -= offset_chi2_vals.min() \n",
    "\n",
    "## Plot the values of the offset that we scanned, and the associated chi^2 values\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(offset_scan_points, offset_chi2_vals, lw=2)\n",
    "\n",
    "ax.set_xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "ax.set_ylabel(r\"$\\Delta \\chi^2$\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4a373",
   "metadata": {},
   "source": [
    "Ok, now let's scan the slope parameter, $p_1$.  We will scan from -2 to 2 counts / year in 11 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a number of different parameter values to scan over\n",
    "nscan = 51\n",
    "\n",
    "## Start with both parameters set to zero\n",
    "params = np.array([0., 0.])\n",
    "\n",
    "## Make a list in which to store the chi^2 values for each parameter set\n",
    "slope_chi2_vals = np.zeros((nscan))\n",
    "\n",
    "## Make an array of offsets to scan over, and loop over each offset\n",
    "slope_scan_points = np.linspace(-2., 2., nscan)\n",
    "for i in range(nscan):\n",
    "    params[1] = slope_scan_points[i]\n",
    "    slope_chi2_vals[i] = chi2_function(years_since_mid_2014, excess_counts, \n",
    "                                        sigma_counts, linear_function, params)\n",
    "\n",
    "## Subtract the minimum value from the computed chi^2 values to get\n",
    "## the \"delta chi^2\" values. We'll assess the minimum value later, \n",
    "## since here we're concerned with how the chi^2 changes as we vary\n",
    "## the offset parameter.\n",
    "slope_chi2_vals -= slope_chi2_vals.min() \n",
    "\n",
    "## Plot the values of the offset that we scanned, and the associated chi^2 values\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(slope_scan_points, slope_chi2_vals, lw=2)\n",
    "\n",
    "ax.set_xlabel(r\"$p_{1}$ = Slope [counts/year]\")\n",
    "ax.set_ylabel(r\"$\\Delta \\chi^2$\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6017017",
   "metadata": {},
   "source": [
    "Now we have found $p_0,p_1$ values which minimize our $\\chi^2$ metric.\n",
    "\n",
    "As always, we are interested in the uncertainty associated with these values. Luckily, we have constructed the $\\chi^2$ by summing together in quadrature a bunch of quantities that **in an ideal world**, are each distributed as a unit gaussian.  This gives us a really powerful tool to interpret changes in the $\\chi^2$ value in terms of Gaussian uncertainties. \n",
    "\n",
    "If we are varying a **single** parameter, we can interpret changes in the parameter that result in a 1 unit change of $\\chi^2$ as the $1 \\sigma$ uncertainty on that parameter.  In general, if $\\hat{p}$ is the set of parameter values that minimize the $\\chi^2$, then we can solve for the uncertainty $\\sigma_p$ using the relationship: \n",
    "\n",
    "$\\Delta \\chi^2 = \\chi^2(\\hat{p} + n \\sigma_p) - \\chi^2(\\hat{p}) = n^2$\n",
    "\n",
    "Where we can pick $n$ to be some convenient number, e.g., 1 in our case.\n",
    "\n",
    "(With more parameters, $\\pm 1\\sigma_p$ uncertainties correspond to > 1 unit change of $\\Delta \\chi^2$, but we can ignore this for now and keep it in the back of our minds. Note that $\\chi^2$ also depends on the data points, and the uncertainties on the data points, but we left that out of the equation to focus on the fact that we are just changing the model parameters and trying to match the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6033a21",
   "metadata": {},
   "source": [
    "### Question for Discussion:\n",
    "\n",
    "#### 7.1 Estimate numerical values for the best-fit values and $1\\sigma$ uncertainties on $p_0$ and $p_1$ using the two plots above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a151e7",
   "metadata": {},
   "source": [
    "### Extracting multiple parameters at once.\n",
    "\n",
    "Now we are going to extract best-fit values for both $p_0$ and $p_1$ by evaluting the $\\chi^2$ function over points on a two dimensional grid.\n",
    "\n",
    "We plot the result as a color plot, where the color is the $\\Delta \\chi^2$ value, and we are going to draw some contours on the plot correspond to different levels of $\\Delta \\chi^2$.\n",
    "\n",
    "This entire process is essentially what a numerical minimizer does, but we're doing it manually to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295c014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Alternate number of points to scan in this 2D implementation\n",
    "nx = 101\n",
    "ny = 101\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_2d_scan_vals = np.zeros((nx, ny))\n",
    "\n",
    "## Scan the offset between -10 and 10, and the slope between -5 and 5\n",
    "offset_2d_scan_points = np.linspace(-10, 10, nx)\n",
    "slope_2d_scan_points = np.linspace(-5, 5, ny)\n",
    "\n",
    "## Double loop for 2d scan, sampling the value of chi^2 at each combination of \n",
    "## parameter values: offset and slope\n",
    "for i in range(nx):\n",
    "    params[0] = offset_2d_scan_points[i]\n",
    "    for j in range(ny):\n",
    "        params[1] = slope_2d_scan_points[j]\n",
    "\n",
    "        chi2_2d_scan_vals[i,j] = chi2_function(years_since_mid_2014, excess_counts, sigma_counts,\n",
    "                                               linear_function, params)\n",
    "\n",
    "## Subtract the minimum chi2 value to get the \"delta chi^2\"\n",
    "min_chi2 = chi2_2d_scan_vals.min()\n",
    "chi2_2d_scan_vals -= min_chi2\n",
    "\n",
    "## This next bit gets the x and y axis values for the grid point at the minimum.\n",
    "idx = chi2_2d_scan_vals.argmin()\n",
    "idx_x = idx//nx\n",
    "idx_y = idx%ny\n",
    "\n",
    "## Get the values of the offset and slope associated to the minimum chi^2\n",
    "scan_min_x = offset_2d_scan_points[idx_x]\n",
    "scan_min_y = slope_2d_scan_points[idx_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's plot it!\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "## Plot the chi^2 values as a 2D image, with the color scale indicating the value of chi^2\n",
    "img = ax.imshow(chi2_2d_scan_vals.T, extent=(-10, 10, -5, 5), aspect='auto', cmap='cividis')\n",
    "fig.colorbar(img, label=r\"$\\Delta \\chi^2$\")\n",
    "\n",
    "## Plot some contours associated to different values of delta chi^2, and thus\n",
    "## different uncertainties on the computed parameters. We'll plot contours for\n",
    "## 1, 4, and 9, which correspond to 1, 2, and 3 sigma uncertainties\n",
    "ax.contour(offset_2d_scan_points, slope_2d_scan_points, chi2_2d_scan_vals.T, levels=[1, 4, 9], colors=\"white\")\n",
    "\n",
    "## Plot the best fit point from our manual scan\n",
    "ax.scatter(scan_min_x, scan_min_y, color='red', marker='x', s=100)\n",
    "\n",
    "ax.set_xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "ax.set_ylabel(r\"$p_1$ = Slope [counts/year]\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best fit value is {min_chi2:0.1f} for parameters ({scan_min_x:0.2f}, {scan_min_y:0.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d4fbea",
   "metadata": {},
   "source": [
    "Note that the X marking the \"best fit\" appears slightly offcenter from the ellipse. This is because we found this point manually by scanning a limited number of parameter values. A numerical minimizer on a computer would scan dynamically, decreasing step size and increasing resolution as it got closer and closer to the true minimum. This is essentially what the `scipy.optimize.curve_fit` function does!\n",
    "\n",
    "There are also entire libraries/packages dedicated to this endeavor, such as [iminuit](https://scikit-hep.org/iminuit/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae58c08",
   "metadata": {},
   "source": [
    "# Step 6, statistical significance of fit values\n",
    "\n",
    "By scanning in a more optimized way, we can get a smoother curve and find a \"best-fit\" value for the slope parameter $p_1$ given by $\\hat{p}_1 = 0.13\\pm0.63$\n",
    "\n",
    "Similarly to what we did last week, we can use `scipy.stats.norm` to compute the p-value.  Once slight difference is that this time we are considering both positive and negative values, so we integrate the Gaussian outward starting from the middle, using the formula in the cell below. The hardcoded factor of 2 comes from the fact that we need to count both the positive and negative tails of the distribution, but the `.sf()` method just integrates up from a positive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9515db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sided_p_chi2(x, mu, sigma):\n",
    "    return 2*(stats.norm(loc=mu, scale=sigma).sf(np.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aff7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = two_sided_p_chi2(0.13, 0, 0.58)\n",
    "print(f\"The p-value to obtain a measurement of {0.13:0.2f} given a true value of {0:0.2f} and a sigma of {0.63:0.2f} is {p_val:0.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bb12ccb",
   "metadata": {},
   "source": [
    "### Question for Discussion:\n",
    "\n",
    "#### 8.1  Is this result statistically significant?  I.e., should we write a paper saying that Vela is getting brighter?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a88bd4",
   "metadata": {},
   "source": [
    "### Quick Coding Exercise for Practice:\n",
    "\n",
    "##### What is the p-value associated to our measured constant offset? A more precise fit yields $\\hat{p}_0 = -1.7 \\pm 2.3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76966aa2",
   "metadata": {},
   "source": [
    "# Summary of this notebook\n",
    "\n",
    "So, to investigate if the correlation was significant we did a few things:\n",
    "\n",
    "1. A **model** with **parameters** to describe the data.  In our case we fit the excess as a function of time to a line,  i.e., $m_{\\rm excess}(t | p_0, p_1) = p_0 + t * p_1$ \n",
    "\n",
    "2. A function to compute how close the model is to each data point.   I.e., how well the model fits the data.  To do this we will first compute the **residuals** for each data point: $\\delta_i = n_{\\rm ex,i} - m_{\\rm ex,i} = n_{\\rm ex,i} - (p_0  + t * p_1)$ \n",
    "\n",
    "3. To interpret the residuals we **scaled** them by their uncertainties, in this case those are the uncertainties shown in the plot above, which are just the square root of the number of observed counts, let's call these $\\sigma_i = \\sqrt{n_{obs, i}}$,  Then we have  $\\chi_i =  \\frac{n_{\\rm ex,i} - m_{\\rm ex,i}}{\\sigma_{obs, i}} = \\frac{n_{\\rm ex,i} - (p_0  + t_i * p_1)}{\\sigma_{obs, i}}$   At that point, if the model were perfect the scaled residuals $\\chi_i$ should each be a variable sampled from a unit Gaussian, i.e., $G(\\chi_i | \\mu=0, \\sigma=1)$.\n",
    "\n",
    "4. The next step was to **test** how well the model fits the data.  Since we have a bunch of data points that ideally should be represented by Gaussians, we can add the variances in quadrature to get a meaningful quantity.  $\\chi^2 = \\sum_i \\chi_i^2 = \\sum_i (\\frac{n_{\\rm ex,i} - (p_0  + t_i * p_1)}{\\sigma_{obs, i}})^2$.   The $\\chi^2$ tells us how well the model fits the data, on average we expect each data point to contribute about 1 unit to the total $\\chi^2$ (because each point is coming from a unit Gaussian).   Lower $\\chi^2$ represent better fits.\n",
    "\n",
    "5. We varied the input parameters $(p_0, p_1)$ to find the values that **minimize** the $\\chi^2$.\n",
    "\n",
    "6. Finally, compared the change in $\\chi^2$ between the best fit model and other parameter values and use this to intepret the statistical significance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920ae35",
   "metadata": {},
   "source": [
    "# Bonus (optional, not for credit)\n",
    "\n",
    "Here is a small coding exercise for you to try. \n",
    "\n",
    "An optimized version of what we covered in this notebook is already implemented in the `scipy.optimize.curve_fit` function. Here is an example of how to fit the model parameters and extract the values and uncertainties using this package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, p0, p1):\n",
    "    return p0 + x*p1\n",
    "\n",
    "\n",
    "popt, pcov = curve_fit(func, years_since_mid_2014, excess_counts, sigma=sigma_counts)\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "print(\"         Optimized parameters: \", popt)\n",
    "print(\"Uncertainty on the parameters: \", perr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acba441",
   "metadata": {},
   "source": [
    "#### 9.1 (Optional coding exercise)  Try fitting the data to a second order polynomial (i.e. $f(x)=p_0 + p_1*x + p_2*x^2$).  Make a plot, with labels and a legend, that shows both the fitted function and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02056574",
   "metadata": {},
   "source": [
    "Tips: Read through the page on [scipy.optimize.curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html). The examples at the bottom are particularly helpful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kipac_physics89L",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

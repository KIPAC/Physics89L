{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e332f5",
   "metadata": {},
   "source": [
    "# Introduction to model fitting\n",
    "\n",
    "### Goals:\n",
    "\n",
    "1. To understand what it means to fit a model, and to derive estimates for the values on the parameters in the model.\n",
    "2. To fit a linear model for the time variation of the flux of the Vela pulsar to the data.\n",
    "3. To use the results of that fit to decide if the flux of the Vela pulsar is, in fact, varying with time.\n",
    "\n",
    "### Timing\n",
    "\n",
    "1. Try to finish this notebook in 35-40 minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531c449",
   "metadata": {},
   "source": [
    "### New functions we will use in this module\n",
    "\n",
    "| Function Name            | What it does |\n",
    "| - | - |\n",
    "|  plt.contour |  Make a contour plot, ie., show the contours correspond to a series of values |\n",
    "| scipy.stats.chi2 | Interact with a $\\chi^2$ distribution, e.g., to compute a p-value |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ede43d",
   "metadata": {},
   "source": [
    "### This figure summarizes the essence of this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f551d1",
   "metadata": {},
   "source": [
    "<img src=\"figures/Fitting.png\" alt=\"drawing\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df2c8f",
   "metadata": {},
   "source": [
    "### Ok, let's pick up where we left off with the Vela pulsar data!\n",
    "\n",
    "(This cell is just a repeat of loading the Vela data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b70a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Vela_Flux.txt\", 'rb'), usecols=range(7))\n",
    "\n",
    "# This is how we pull out the data from columns in the array.\n",
    "\n",
    "# This is the date in \"Mission Elapsesd Time\"\n",
    "# For the Fermi mission, this is defined to be the number of seconds since the start of 2001.\n",
    "date_MET = data[:,0]\n",
    "# This is the offset in seconds between the Fermi \"MET\" and the UNIX \"epoch\" used by matplotlib\n",
    "MET_To_Unix = 978336000\n",
    "\n",
    "# These are the numbers of photons observed from Vela each week in the \"low\" Energy Band (100 MeV - 800 MeV)\n",
    "nObs_LE = data[:,1]\n",
    "\n",
    "# These are the number of photons expected from Vela each week, under the assumption that it is \n",
    "# not varying at all, and the only differences depend on how long we spent looking at Vela\n",
    "# that particular weeek\n",
    "nExp_LE = data[:,2]\n",
    "\n",
    "# These are the band bounds, in MeV\n",
    "LE_bounds = (100., 800.)\n",
    "\n",
    "# This is the \"significance\" of the variation for each week.  We will discuss this more later\n",
    "signif_LE = data[:,3]\n",
    "\n",
    "nObs_HE = data[:,4]\n",
    "nExp_HE = data[:,5]\n",
    "signif_HE = data[:6]\n",
    "HE_bounds = (800., 10000.)\n",
    "\n",
    "# This converts the dates to something that matplotlib understands\n",
    "dates = [datetime.datetime.fromtimestamp(date + MET_To_Unix) for date in date_MET]\n",
    "date_YEAR = 2001 +  (date_MET / (24*3600*365))\n",
    "years_since_mid_2014 = date_YEAR  - 2014.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0a1cc",
   "metadata": {},
   "source": [
    "So, we are going to investigate if the small correlation between the excess and the time is statistically significant.   Let's start by looking at the data and adding error bars to the points and plotting it using the variables we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "excess_counts = nObs_LE-nExp_LE\n",
    "sigma_counts =  np.sqrt(nObs_LE)\n",
    "plt.errorbar(years_since_mid_2014, excess_counts, fmt='ko', ms=4, ecolor='pink', yerr=sigma_counts)\n",
    "plt.xlabel(r\"Date w.r.t mid 2014 [years]\")\n",
    "plt.ylabel(r\"$n_{\\rm obs}$ [per week]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d696e5",
   "metadata": {},
   "source": [
    "# Step 1.   The model\n",
    "\n",
    "We just write a simple function for our model, and plot how the model looks for a few values of the parameters. Call $p_0$ and $p_1$ our parameters, representing the offset and slope in a linear model, respectively. Then our model for time dependence, given these parameters, is:\n",
    "\n",
    "$m_{\\rm ex}(t | p_0, p_1) = p_0 + t * p_1$ \n",
    "\n",
    "In the next cell we will define a function that returns the model prediction for the y-values (i.e., excess counts) if you pass in a set of x-values (i.e., times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b798ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(xvals, params):\n",
    "    return params[0] + xvals*params[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473732d",
   "metadata": {},
   "source": [
    "Now we are going to plot that model for a few different sets of paramters.\n",
    "\n",
    "First we will change $p_0$, the offset parameter, and plot that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(r\"Time since mid 2014 [years]\")\n",
    "plt.ylabel(r\"$n_{\\rm ex}$ [per week]\")\n",
    "plt.errorbar(years_since_mid_2014, excess_counts, fmt='ko', ms=4, ecolor='pink', yerr=sigma_counts, alpha=0.4)\n",
    "\n",
    "xvals = years_since_mid_2014\n",
    "params = np.array([0, 0])\n",
    "for offset in np.linspace(-200, 200, 5):\n",
    "    params[0] = offset\n",
    "    plt.plot(xvals, linear_function(xvals, params), label=rf\"Offset = {offset:0.0f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f925401",
   "metadata": {},
   "source": [
    "Now we will change $p_1$, the slope, and plot that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(r\"Time since mid 2014 [years]\")\n",
    "plt.ylabel(r\"$n_{\\rm ex}$ [per week]\")\n",
    "plt.errorbar(years_since_mid_2014, excess_counts, yerr=sigma_counts, alpha=0.2)\n",
    "\n",
    "xvals = years_since_mid_2014\n",
    "params = np.array([0, 0])\n",
    "for slope in np.linspace(-15, 15, 5):\n",
    "    params[1] = slope\n",
    "    plt.plot(xvals, linear_function(xvals, params), label=rf\"Slope = {slope:0.1f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af226cc4",
   "metadata": {},
   "source": [
    "### An important distinction:\n",
    "\n",
    "When we refer to the model, we are referring to the function which states that the number of counts changes linearly with time. A particular *version* of the model is one with a specific set of parameters.  Keep that in mind as you answer the next question, and for the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbc6d3",
   "metadata": {},
   "source": [
    "### Questions for discussion\n",
    "\n",
    "#### 4.1  How well do you think this model describes the data?  Is this a sensible model?   What would it mean if $p_0$ were very different from 0?  What about if $p_1$ were different from zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a78c78",
   "metadata": {},
   "source": [
    "# Step 2.   The residuals\n",
    "\n",
    "Let's try to quantify the agreement of the model with the data.\n",
    "\n",
    "Let's write a function to plot the **residuals** given the model parameters.  \n",
    "\n",
    "Let's define the residuals as: $\\delta_i = n_{\\rm ex,i} - m_{\\rm ex,i} = n_{\\rm ex,i} - (p_0  + t * p_1)$, or in words, the difference between the model predictions and the measured data points.\n",
    "\n",
    "The first function `residual_function` computes the residuals $\\delta_i$ for each week.  \n",
    "\n",
    "The second function `plot_residuals` makes scatter plots of the residuals for different sets of model parameters versus time.\n",
    "\n",
    "The third function `hist_residuals` makes histograms of the residuals for different sets of model parameters over all the weeks. \n",
    "\n",
    "Let's test these functions on two sets of model parameters: one where $(p_0,p_1) = (0,0)$, and one where $(p_0,p_1) = (0,20)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c255a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_function(data_x, data_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    residual = data_y - model_y\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6adcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(xvals, yvals_list):\n",
    "    plt.xlabel(r\"Time since mid 2014 [years]\")\n",
    "    plt.ylabel(r\"$\\delta$ [counts]\")\n",
    "    for yvals in yvals_list:\n",
    "        plt.scatter(xvals, yvals)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_residuals(yvals_list):\n",
    "    plt.xlabel(r\"$\\delta$ [counts]\")\n",
    "    plt.ylabel(\"Weeks [per 10 counts]\")\n",
    "    for yvals in yvals_list:\n",
    "        plt.hist(yvals, bins=np.linspace(-300, 300, 61), histtype='step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5049279",
   "metadata": {},
   "source": [
    "Now we'll take a look at the residuals for 2 sets of parameters: one with $p_1 = 0$, and one with $p_1 = 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_0_0  = residual_function(years_since_mid_2014, excess_counts, linear_function, [0,0])\n",
    "resid_0_20 = residual_function(years_since_mid_2014, excess_counts, linear_function, [0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6032bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(years_since_mid_2014, [resid_0_20, resid_0_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_residuals([resid_0_20, resid_0_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c2cfe",
   "metadata": {},
   "source": [
    "# Step 3.   The scaled residuals\n",
    "\n",
    "We see there's some scatter to the residuals. However, we also know from previous notebooks that the data itself has an inherent spread, which we want to take into account when trying to assess the agreement between a model and data.\n",
    "\n",
    "Let's write a function to plot the **scaled residuals** (i.e.,  the residuals divided by the uncertainties on the data points to scale them down to a unit Gaussian), given the model parameters:\n",
    "\n",
    "$$\\chi_i =  \\frac{n_{\\rm ex,i} - m_{\\rm ex,i}}{\\sigma_{obs, i}} = \\frac{n_{\\rm ex,i} - (p_0  + t_i * p_1)}{\\sigma_{obs, i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_scaled_residuals(yvals_list):\n",
    "    plt.xlabel(r\"$\\delta$ [$\\sigma$]\")\n",
    "    plt.ylabel(r\"Weeks [per $0.2 \\sigma$]\")\n",
    "    for yvals in yvals_list:\n",
    "        plt.hist(yvals, bins=np.linspace(-5, 5, 51), histtype='step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d8dba",
   "metadata": {},
   "source": [
    "#### Here we scale the individual residuals, so that we can make nice scatter plots of the scaled residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e66bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_0_0 = resid_0_0/sigma_counts\n",
    "chi_0_20 = resid_0_20/sigma_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(years_since_mid_2014, [chi_0_20, chi_0_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91086c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_scaled_residuals([chi_0_20, chi_0_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e42b45",
   "metadata": {},
   "source": [
    "###  Questions for discussion.\n",
    "\n",
    "#### 5.1 Using what we learned last week about Gaussian distributions and p-values, explain why the two plots with the scaled residuals are much more useful than the two plots with the unscaled residuals.\n",
    "\n",
    "#### 5.2 Given the plot above, do you think that there is any chance that the true value of $p_1$ is actually 20 counts / year?  Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bd176",
   "metadata": {},
   "source": [
    "# Step 4.   The $\\chi^2$ distribution\n",
    "\n",
    "Let's formulate a single number to summarize these residuals. Let's define: \n",
    "\n",
    "$$f(\\chi_i) = \\sum_i \\chi_i^2,$$\n",
    "\n",
    "where $\\chi_i$ is the scaled residual for data point $i$.\n",
    "\n",
    "The expected distribution of $f(\\chi_i)$ is known, assuming $\\chi_i$ follows a normal distribution centered at zero with $\\sigma$ = 1. It should follow what is known as a $\\chi^2$ distribution (given by the `scipy.stats.chi2` function). \n",
    "\n",
    "What types of numbers are described by the $\\chi^2$ distribution? \n",
    "\n",
    "If we draw values $X_i$ from a normal distribution centered at zero with $\\sigma$ = 1, then the sum of the squares of $X_i$ will follow a $\\chi^2$ distribution. The form of this distribution has only one parameter, which we call the \"degrees of freedom\" (also called $DOF$ or $d_{f}$ in shorthand). The mean of the $\\chi^2$ distribution is $d_f$, and the variance is $2d_f$.\n",
    "\n",
    "Let's explore the distribution a little bit before going further with the Vela pulsar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850ddbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the chi2 distribution for different degrees of freedom\n",
    "x = np.arange(0,10,.05)\n",
    "for df in [1,2,3,4,5]:\n",
    "    plt.plot(x, stats.chi2(df=df).pdf(x), label=rf\"$d_f = {df:d}$\")\n",
    "plt.xlabel('$\\chi_{d_f}^2$')\n",
    "plt.ylabel('Probability')\n",
    "plt.ylim(0,0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207c2bf",
   "metadata": {},
   "source": [
    "In our case, the $\\chi^2$ distribution applies because we are summing the squares of the scaled residuals.\n",
    "\n",
    "As suggested by the name, \"degrees of freedom\" refers to the number of *independent* pieces of information we use when comparing observations to a model. In our case, this means the number of data points we have collected, minus the number of parameters in our model. So for our quantity $\\chi^2$, the degrees of freedom are:\n",
    "\n",
    "$d_f = n - m,$\n",
    "\n",
    "where $n$ is the number of data points and $m$ is the number of fit parameters (so in our case, 2 for $p_0$ and $p_1$).\n",
    "\n",
    "$f(\\chi_i)$ is usually just referred to as $\\chi^2$, or \"chi-squared\", because it follows the $\\chi^2$ distribution. That's how we'll refer to it from now on. Since the mean of the $\\chi^2$ distribution is $d_f$, for a \"good\" fit, $\\chi^2/d_f$ should be approximately 1. \n",
    "\n",
    "Let's calculate some values of $\\chi^2$ for our data and two sets of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_function(data_x, data_y, data_sigma_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    chi2 = ((data_y - model_y)/(data_sigma_y))**2\n",
    "    return np.sum(chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60186d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_0_0 = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, [0,0])\n",
    "chi2_0_20 = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, [0,20])\n",
    "chi2_check = np.sum(chi_0_0*chi_0_0)\n",
    "DOF = chi_0_0.size-2\n",
    "print(f\"Chi-squared(0,0)  : {chi2_0_0:0.1f}\")\n",
    "print(f\"Chi-squared check : {chi2_check:0.1f}\")\n",
    "print(f\"Chi-squared(0,0)/DOF : {chi2_0_0/DOF:0.1f}\\n\")\n",
    "print(f\"Chi-squared(0,20) : {chi2_0_20:0.1f}\")\n",
    "print(f\"Chi-squared(0,20)/DOF : {chi2_0_20/DOF:0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7636c",
   "metadata": {},
   "source": [
    "Using the $\\chi^2$ distribution, we can calculate the likelihood of observing a dataset with the given $\\chi^2$ value if our model parameters are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The odds of seeing the observed data given params (0.,0.) are {stats.chi2(df=DOF).sf(chi2_0_0):.1e}\")\n",
    "print(f\"The odds of seeing the observed data given params (0.,20.) are {stats.chi2(df=DOF).sf(chi2_0_20):.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f66e34b",
   "metadata": {},
   "source": [
    "# Question for discussion.\n",
    "\n",
    "#### 6.1 We just saw that in an idealized case, chance of seeing the observed data given model parameters (0.,20.) is astonishingly less likely (by a factor of $10^{126}$ than the chance of seeing the observed data if the model parameters were (0., 0.).  Explain why this is, using information from the plots above question 5.1 and 5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda8cd5",
   "metadata": {},
   "source": [
    "# Step 5.  Minimizing the $\\chi^2$\n",
    "\n",
    "Now that we have a metric to quantify the difference between the model prediction and the data point, we can use this metric to find the values of $p_0,p_1$ which best describe the model. To do this, we can scan over parameter values and try to minimize the $\\chi^2$ value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a8e38",
   "metadata": {},
   "source": [
    "# Scanning parameters.\n",
    "\n",
    "We are first going to find the parameter values that minimize the $\\chi^2$ by scanning each of the parameters and computing the resulting $\\chi^2$. \n",
    "\n",
    "First we are going to scan each variable on its own.  Then we will do a two dimensional scan.\n",
    "\n",
    "We will do $p_0$ (the offset parameter) first.  We will scan from -5 to 5 counts in 11 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "plt.ylabel(r\"$\\Delta \\chi^2$\")\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_vals = np.zeros((11))\n",
    "offset_scan_points = np.linspace(-5., 5., 11)\n",
    "for i in range(11):\n",
    "    params[0] = offset_scan_points[i]\n",
    "    chi2_vals[i] = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, params)\n",
    "\n",
    "chi2_vals -= chi2_vals.min() # Note that this subtracts the minimum chi-2 value, to get the \"delta chi2\"\n",
    "plt.plot(offset_scan_points, chi2_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4a373",
   "metadata": {},
   "source": [
    "Ok, now let's scan the slope parameter, $p_1$.  We will scan from -2 to 2 counts / year in 11 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(r\"$p_{1}$ = Slope [counts/year]\")\n",
    "plt.ylabel(r\"$\\Delta\\chi^2$\")\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_vals = np.zeros((11))\n",
    "slope_scan_points = np.linspace(-2., 2., 11)\n",
    "for i in range(11):\n",
    "    params[1] = slope_scan_points[i]\n",
    "    chi2_vals[i] = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, params)\n",
    "\n",
    "chi2_vals -= chi2_vals.min() # Note that this subtracts the minimum chi-2 value, to get the \"delta chi2\"\n",
    "plt.plot(slope_scan_points, chi2_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6017017",
   "metadata": {},
   "source": [
    "Now we have found $p_0,p_1$ values which minimize our $\\chi^2$ metric.\n",
    "\n",
    "As always, we are interested in the uncertainty associated with these values. Luckily, we have constructed the $\\chi^2$ by summing together in quadrature a bunch of quantities that **in an ideal world**, are each distributed as a unit gaussian.  This gives us a really powerful tool to interpret changes in the $\\chi^2$ value in terms of Gaussian errors. \n",
    "\n",
    "If we are varying a **single** parameter, we can interpret changes in the parameter that result in a 1 unit change of $\\chi^2$ as the $1 \\sigma$ uncertainty on that parameter.  In general, if $\\hat{p}$ is the set of parameter values that minimize the $\\chi^2$, then we can solve for the uncertainty $\\sigma_p$ using the relationship: \n",
    "\n",
    "$\\Delta \\chi^2 = \\chi^2(\\hat{p} + n \\sigma_p) - \\chi^2(\\hat{p}) = n^2$\n",
    "\n",
    "Where we can pick $n$ to be some convenient number, e.g., 1 in our case.\n",
    "\n",
    "(With more parameters, $\\pm 1\\sigma_p$ errors correspond to > 1 unit change of $\\Delta \\chi^2$, but we can ignore this for now and keep it in the back of our minds. Note that $\\chi^2$ also depends on the data points, and the errors on the data points, but we left that out of the equation to focus on the fact that we are just changing the model parameters and trying to match the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6033a21",
   "metadata": {},
   "source": [
    "### Quick Question\n",
    "\n",
    "#### 7.1 Estimate numerical values for the best-fit values and $1\\sigma$ uncertainties on $p_0$ and $p_1$ using the two plots above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a151e7",
   "metadata": {},
   "source": [
    "### Extracting multiple parameters at once.\n",
    "\n",
    "Now we are going to extract best-fit values for both $p_0$ and $p_1$ by evaluting the $\\chi^2$ function over points on a two dimensional grid.\n",
    "\n",
    "We plot the result as a color plot, where the color is the $\\Delta \\chi^2$ value, and we are going to draw some contours on the plot correspond to different levels of $\\Delta \\chi^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295c014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "plt.ylabel(r\"$p_1$ = Slope [counts/year]\")\n",
    "\n",
    "\n",
    "nx = 51\n",
    "ny = 51\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_2d_scan_vals = np.zeros((nx, ny))\n",
    "offset_scan_points = np.linspace(-10, 10, nx)\n",
    "slope_scan_points = np.linspace(-5, 5, ny)\n",
    "\n",
    "# Double loop for 2d scan\n",
    "for i in range(nx):\n",
    "    params[0] = offset_scan_points[i]\n",
    "    for j in range(ny):\n",
    "        params[1] = slope_scan_points[j]\n",
    "        chi2_2d_scan_vals[i,j] = chi2_function(years_since_mid_2014, excess_counts, sigma_counts,\n",
    "                                               linear_function, params)\n",
    "\n",
    "min_chi2 = chi2_2d_scan_vals.min()\n",
    "chi2_2d_scan_vals -= min_chi2\n",
    "\n",
    "# This next bit gets the x and y axis values for the grid point at the minimum.\n",
    "idx = chi2_2d_scan_vals.argmin()\n",
    "idx_x = idx//nx\n",
    "idx_y = idx%ny\n",
    "scan_min_x = offset_scan_points[idx_x]\n",
    "scan_min_y = offset_scan_points[idx_y]\n",
    "\n",
    "# Now let's plot it.\n",
    "plt.imshow(chi2_2d_scan_vals.T, extent=(-10, 10, -5, 5), aspect='auto')\n",
    "plt.colorbar(label=r\"$\\Delta \\chi^2$\")\n",
    "plt.contour(offset_scan_points, slope_scan_points, chi2_2d_scan_vals.T, levels=[1, 4, 9], colors=\"white\")\n",
    "plt.scatter(scan_min_x, scan_min_y)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best fit value is {min_chi2:0.1f} for ({scan_min_x:0.1f} {scan_min_y:0.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae58c08",
   "metadata": {},
   "source": [
    "# Step 6, statistical significance of fit values\n",
    "\n",
    "By scanning in a more optimized way, we can get a smoother curve and find a value of 0.13 +- 0.63 for $p_1$, the slope of the model line.\n",
    "\n",
    "Similarly to what we did last week, we can use `scipy.stats.norm` to compute the p-value.  Once slight difference is that this time we are considering both positive and negative values, so we integrate the Gaussian outward starting from the middle, using the formula in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9515db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sided_p_chi2(x, mu, sigma):\n",
    "    return 2*(stats.norm(loc=mu, scale=sigma).sf(np.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aff7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = two_sided_p_chi2(0.13, 0, 0.58)\n",
    "print(f\"The p-value to obtain a measurement of {0.13:0.2f} given a true value of {0:0.2f} and a sigma of {0.63:0.2f} is {p_val:0.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bb12ccb",
   "metadata": {},
   "source": [
    "### Quick question.\n",
    "\n",
    "#### 8.1  Is this result statistically significant?  I.e., should we write a paper saying that Vela is getting brighter?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76966aa2",
   "metadata": {},
   "source": [
    "# Summary of this notebook\n",
    "\n",
    "So, to investigate if the correlation was significant we did a few things:\n",
    "\n",
    "1. A **model** with **parameters** to describe the data.  In our case we fit the excess as a function of time to a line,  i.e., $m_{\\rm ex}(t | p_0, p_1) = p_0 + t * p_1$ \n",
    "\n",
    "2. A function to compute how close the model is to each data point.   I.e., how well the model fits the data.  To do this we will first compute the **residuals** for each data point: $\\delta_i = n_{\\rm ex,i} - m_{\\rm ex,i} = n_{\\rm ex,i} - (p_0  + t * p_1)$ \n",
    "\n",
    "3. To interpret the residuals we **scaled** them by their errors, in this case that is the error bars in the plot above, which are just the square root of the number of observed counts, let's call these $\\sigma_i = \\sqrt{n_{obs, i}}$,  Then we have  $\\chi_i =  \\frac{n_{\\rm ex,i} - m_{\\rm ex,i}}{\\sigma_{obs, i}} = \\frac{n_{\\rm ex,i} - (p_0  + t_i * p_1)}{\\sigma_{obs, i}}$   At that point, if the model were perfect the scaled residuals $\\chi_i$ should each be a variable sampled from a unit Gaussian, i.e., $G(\\chi_i | \\mu=0, \\sigma=1)$.\n",
    "\n",
    "4. The next step was to **test** how well the model fits the data.  Since we have a bunch of data points that ideally should be represented by Gaussians, we can add the variances in quadrature to get a meaningful quantity.  $\\chi^2 = \\sum_i \\chi_i^2 = \\sum_i (\\frac{n_{\\rm ex,i} - (p_0  + t_i * p_1)}{\\sigma_{obs, i}})^2$.   The $\\chi^2$ tells us how well the model fits the data, on average we expect each data point to contribute about 1 unit to the total $\\chi^2$ (because each point is coming from a unit Gaussian).   Lower $\\chi^2$ represent better fits.\n",
    "\n",
    "5. We varied the input parameters $(p_0, p_1)$ to find the values that **minimize** the $\\chi^2$.\n",
    "\n",
    "6. Finally, compared the change in $\\chi^2$ between the best fit model and other parameter values and use this to intepret the statistical significance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920ae35",
   "metadata": {},
   "source": [
    "# Bonus (optional, not for credit)\n",
    "\n",
    "By popular request, here is a small coding exercise for you to try. \n",
    "\n",
    "An optimized version of what we covered in this notebook is already implemented in the `scipy.optimize.curve_fit` function. Here is an example of how to fit the model parameters and extract the values and errors using this package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, p0, p1):\n",
    "    return p0 + x*p1\n",
    "\n",
    "\n",
    "popt, pcov = curve_fit(func, years_since_mid_2014, excess_counts, sigma=sigma_counts)\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "print(\"Optimized parameters:   \", popt)\n",
    "print(\"Error on the parameters:\",perr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acba441",
   "metadata": {},
   "source": [
    "#### 9.1 (Optional coding exercise)  Try fitting the data to a second order polynomial (i.e. $f(x)=p_0 + p_1*x + p_2*x^2$).  Make a plot, with labels and a legend, that shows both the fitted function and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02056574",
   "metadata": {},
   "source": [
    "Tips: Read through the page on [scipy.optimize.curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html). The examples at the bottom are particularly helpful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3887470a",
   "metadata": {},
   "source": [
    "# Using minimizers and the parameter choice in model fitting\n",
    "\n",
    "### Goals:\n",
    "\n",
    "1. To continue fitting models of time-variability to the Vela pulsar data.\n",
    "2. To understand how our choice of model parameter can affect the quality of the results.\n",
    "\n",
    "### Timing\n",
    "\n",
    "1. Try to finish this notebook in 35-40 minutes. \n",
    "\n",
    "### Question and Answer Template\n",
    "\n",
    "You can go to the link below, and do \"file\" -> \"make a copy\" to make yourself a google doc that you can use to fill in the answers to the question in this weeks notebooks.\n",
    "\n",
    "https://docs.google.com/document/d/1Q2jJXXkg0rISnucJdY6voYC9vC_xVOljSD6p3Mm4FiY/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e711ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "import datetime\n",
    "\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7dc1a",
   "metadata": {},
   "source": [
    "### New functions we will use in this module\n",
    "\n",
    "| Function Name            | What it does |\n",
    "| - | - |\n",
    "| scipy.optimize.minimize  | Find the set of parameters that minimize a function |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ee46b",
   "metadata": {},
   "source": [
    "### Ok let's pick up where we left off with the Vela pulsar data\n",
    "\n",
    "(This cell is just a repeat of loading the Vela data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b370ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Vela_Flux.txt\", 'rb'), usecols=range(7))\n",
    "\n",
    "## This is how we pull out the data from columns in the array.\n",
    "\n",
    "## This is the date in \"Mission Elapsed Time\" For the Fermi mission, this \n",
    "## is defined to be the number of seconds since the start of 2001.\n",
    "date_MET = data[:,0]\n",
    "\n",
    "## This is the offset in seconds between the Fermi \"MET\" and the UNIX \n",
    "## \"epoch\" used by matplotlib\n",
    "MET_To_Unix = 978336000\n",
    "\n",
    "## These are the number of photons observed from Vela each week in \n",
    "## the \"low\" Energy Band (100 MeV - 800 MeV)\n",
    "nObs_LE = data[:,1]\n",
    "\n",
    "## These are the number of photons expected from Vela each week, under \n",
    "## the assumption that it is not varying at all, and the only differences \n",
    "## depend on how long we spent looking at Vela that particular weeek\n",
    "nExp_LE = data[:,2]\n",
    "\n",
    "## These are the band bounds, in MeV\n",
    "LE_bounds = (100., 800.)\n",
    "\n",
    "## We will also take a look at data in the \"high\" energy band \n",
    "## (800 MeV - 10000 MeV)\n",
    "nObs_HE = data[:,4]\n",
    "nExp_HE = data[:,5]\n",
    "signif_HE = data[:6]\n",
    "HE_bounds = (800., 10000.)\n",
    "\n",
    "## This converts the dates to something that matplotlib understands\n",
    "dates = [datetime.datetime.fromtimestamp(date + MET_To_Unix) for date in date_MET]\n",
    "\n",
    "## Convert the dates to years to make the numbers more sensible\n",
    "date_YEAR = 2001 +  (date_MET / (24*3600*365))\n",
    "years_since_mid_2014 = date_YEAR  - 2014.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565f2dd",
   "metadata": {},
   "source": [
    "### Now let's add the functions we needed to minimize the $\\chi^2$\n",
    "\n",
    "That includes the model, a function to calculate the residuals and a function to compute the $\\chi^2$.\n",
    "\n",
    "These are exactly the same functions that we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(xvals, params):\n",
    "    return params[0] + xvals*params[1]\n",
    "\n",
    "## Function to calculate an array of residuals as (data) - (model)\n",
    "def residual_function(data_x, data_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    residual = data_y - model_y\n",
    "    return residual\n",
    "\n",
    "## Compute the chi-squared test statistic for a particular data set, \n",
    "## model, and set of parameters. We'll minimize this statistic as a \n",
    "## function of the model parameters.\n",
    "def chi2_function(data_x, data_y, data_sigma_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    chi2 = ((data_y - model_y)/(data_sigma_y))**2\n",
    "    return np.sum(chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b5843",
   "metadata": {},
   "source": [
    "# Function minimizers\n",
    "\n",
    "Finding the set of parameters that minimize a function is a very common thing to do in research. In our case, we are looking for the set of model parameters that give us the smallest $\\chi^2$, i.e., the best fit.\n",
    "\n",
    "Last week, we did this by hand by varying two parameters in a linear model and calculating the $\\chi^2$ for each set of values. \n",
    "\n",
    "Since this is such a common thing to do, there are many software packages that will do it.  Typically they refer to the function that is being minimized as the \"cost function\", and they expect you to provide a function that takes only the model parameters as inputs.\n",
    "\n",
    "So we are going to write a \"cost function\" that just calls our $\\chi^2$ with the right versions of the data and model.  \n",
    "\n",
    "Because the software we use (`scipy.optimize`) has a slightly different convention than what we are using, we will multiply the $\\chi^2$ by a factor of 0.5 so that the uncertainty estimated returned by the minimizer will be correct.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b838578",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate our excess counts from the observed and expected data.\n",
    "## This will be the dataset to which we fit a linear model\n",
    "excess_counts = nObs_LE-nExp_LE\n",
    "\n",
    "## Assign poisson-like uncertainties to the measured exess counts.\n",
    "sigma_counts =  np.sqrt(nObs_LE)\n",
    "\n",
    "## Define a cost function that takes only model parameters as input\n",
    "## and returns the chi-squared statistic.\n",
    "def cost_function(params):\n",
    "    return 0.5*chi2_function(years_since_mid_2014, excess_counts, sigma_counts, \n",
    "                             linear_function, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff78663",
   "metadata": {},
   "source": [
    "### Invoking the minimizer and looking at the result\n",
    "\n",
    "First we are going to invoke the minimizer in the next cell.\n",
    "\n",
    "It is worth spending a bit of time thinking about what it is doing.\n",
    "\n",
    "Note that we pass three things to the minimizer:\n",
    "   1. the `cost_function` (you might not have explicitly noticed this before, but we can pass functions to other functions)\n",
    "   2. an initial guess as to the parameter values.  In this case we will start with (0., 0.), i.e., slope and offset are both zero.\n",
    "   3. The method of minimization. There are many algorithms that have been developed to minimize functions. Here we choose one of the speedier algorithms, but you can specify different algorithms, or allow SciPy to make a choice for you based different optional arguments you might pass to the function. (see [scipy reference](https://docs.scipy.org/doc/scipy/reference/optimize.html)).\n",
    "   \n",
    "And note that the minimizer returns a `result` object to us, which we will explore in the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = optimize.minimize(cost_function, [0., 0.], method=\"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e105b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the best fit parameters and the value of the cost \n",
    "## function at the minimum. The 'result' is a dictionary with \n",
    "## many different keys\n",
    "pars = result['x']\n",
    "fmin = result['fun']\n",
    "p0_best = pars[0]\n",
    "p1_best = pars[1]\n",
    "\n",
    "## Extract the covariance in parameters, as estimated from the\n",
    "## inverse of the Hessian matrix.\n",
    "cov = result['hess_inv']\n",
    "\n",
    "## Compute uncertainties as the square root of the diagonal\n",
    "## elements of the covariance matrix, and then compute the\n",
    "## correlation of the parameters based on these values\n",
    "p0_err = np.sqrt(cov[0,0])\n",
    "p1_err = np.sqrt(cov[1,1])\n",
    "correl = cov[1,0]/np.sqrt(p0_err*p1_err)\n",
    "\n",
    "print(\"Fitter result:\")\n",
    "print(result)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Human readable version ---------------\")\n",
    "print(f\"               p0 best fit value: {p0_best:.1f} ± {p0_err:.1f} counts\")\n",
    "print(f\"               p1 best fit value: {p1_best:.1f} ± {p1_err:.1f} counts / year\")\n",
    "print(f\"  Minimum value of cost function: {fmin:.1f}\")\n",
    "print(f\"         Minimum value of chi**2: {(2*fmin):.1f}\")\n",
    "print(f\"   Correlation between p0 and p1: {correl:.2f}\")\n",
    "print(f\"  Number of times cost function was evaluated to find minimum: {result['nfev']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d8d5f",
   "metadata": {},
   "source": [
    "### Ok, now let's draw the fit result on top of the contours we made last week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad538b40",
   "metadata": {},
   "source": [
    "This first cell is just a copy of something we did last week, although the number of manual scan points has been reduced to better illustrate the inaccuracy as compared to the numerical minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternate number of points to scan in this 2D implementation\n",
    "nx = 21\n",
    "ny = 21\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_2d_scan_vals = np.zeros((nx, ny))\n",
    "\n",
    "## Scan the offset between -10 and 10, and the slope between -5 and 5\n",
    "offset_2d_scan_points = np.linspace(-10, 10, nx)\n",
    "slope_2d_scan_points = np.linspace(-5, 5, ny)\n",
    "\n",
    "## Double loop for 2d scan, sampling the value of chi^2 at each combination of \n",
    "## parameter values: offset and slope\n",
    "for i in range(nx):\n",
    "    params[0] = offset_2d_scan_points[i]\n",
    "    for j in range(ny):\n",
    "        params[1] = slope_2d_scan_points[j]\n",
    "\n",
    "        chi2_2d_scan_vals[i,j] = chi2_function(years_since_mid_2014, excess_counts, sigma_counts,\n",
    "                                               linear_function, params)\n",
    "\n",
    "## Subtract the minimum chi2 value to get the \"delta chi^2\"\n",
    "min_chi2 = chi2_2d_scan_vals.min()\n",
    "chi2_2d_scan_vals -= min_chi2\n",
    "\n",
    "## This next bit gets the x and y axis values for the grid point at the minimum.\n",
    "idx = chi2_2d_scan_vals.argmin()\n",
    "idx_x = idx//nx\n",
    "idx_y = idx%ny\n",
    "\n",
    "## Get the values of the offset and slope associated to the minimum chi^2\n",
    "scan_min_x = offset_2d_scan_points[idx_x]\n",
    "scan_min_y = slope_2d_scan_points[idx_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192f5a4",
   "metadata": {},
   "source": [
    "This second cell is almost a copy of the plot from last week, but with numerical minimizer result also plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's plot it!\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "## Plot the chi^2 values as a 2D image, with the color scale \n",
    "## indicating the value of chi^2\n",
    "img = ax.imshow(chi2_2d_scan_vals.T, extent=(-10, 10, -5, 5), origin='lower', aspect='auto', cmap='cividis')\n",
    "fig.colorbar(img, label=r\"$\\Delta \\chi^2$\")\n",
    "\n",
    "## Plot some contours associated to different values of delta chi^2, \n",
    "## and thus different uncertainties on the computed parameters. We'll \n",
    "## plot contours for 1, 4, and 9, which correspond to 1, 2, and 3 \n",
    "## sigma uncertainties\n",
    "ax.contour(offset_2d_scan_points, slope_2d_scan_points, chi2_2d_scan_vals.T, levels=[1, 4, 9], colors=\"white\")\n",
    "\n",
    "## Plot the manual best-fit point\n",
    "ax.scatter(scan_min_x, scan_min_y, color='red', marker='x', s=50, label=\"manual\")\n",
    "\n",
    "## Plot the NEW fit point from our numerically minimized\n",
    "plt.errorbar(p0_best, p1_best, xerr=p0_err, yerr=p1_err, color='cyan', label=\"minimized\")\n",
    "\n",
    "ax.set_xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "ax.set_ylabel(r\"$p_1$ = Slope [counts/year]\")\n",
    "\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c4748",
   "metadata": {},
   "source": [
    "### Questions for discusion\n",
    "\n",
    "#### 1.1  Let's make sure that you understand what we have done so far. Describe in some detail the relationship between the colormap, the white contours, the cyan error bars, and the red X. What does each thing represent physically? How do they correspond to the information in the \"human readable\" printout of the result?  (Note that this is the same colormap that we made at the end of last week's second notebook when we did the two dimensional scan over the parameter values.)\n",
    "\n",
    "#### 1.2 To make the colormap we had to do a double loop over a grid of points for the parameters $p_0$ and $p_1$. The grid was 21x21, for a total of 441 points, meaning we had to evaluate the cost function 441 times. In this relatively simple example, we only had 2 parameters; imagine instead that we had 3 or 4 parameters. How would that affect the time it took to evaluate the cost function on a grid over all the parameters? Compare that to the number of calls that the fitter makes in order to find the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c18be9",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "\n",
    "Let's play around with the fitter and try it with 10 different initial guesses and compare the results and the number of calls to the cost function it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop over 10 different random starting points, and do the minimization\n",
    "## for each of them and print the result.\n",
    "for i in range(10):\n",
    "\n",
    "    ## Generate random starting point, sampling the constant from a \n",
    "    ## uniform distribution between -10 and 10, and the slope from a\n",
    "    ## uniform distribution between -5 and 5\n",
    "    x0 = [np.random.uniform(-10, 10), np.random.uniform(-5, 5)]\n",
    "\n",
    "    ## Optimize our cost function and extract the best fit parameters\n",
    "    result = optimize.minimize(cost_function, x0=x0)\n",
    "    best_fit = result['x']\n",
    "\n",
    "    ## Print the best fit parameters and the number of times the cost\n",
    "    ## function was evaluated to find the minimum.\n",
    "    print(f\"Initial guess: ({x0[0]:+.2f} {x0[1]:+.2f}): found fit result: ({best_fit[0]:.6f} {best_fit[1]:.6f}) after {result['nfev']} calls to the cost function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241edda8",
   "metadata": {},
   "source": [
    "### Question for discussion\n",
    "\n",
    "#### 2.1 The results for the 10 trials are very similar but not quite identical. Does this make sense to you? Do you have an idea about why the results aren't identical? How do you think the fitter decides that it is done? (Don't worry if you don't know the answer, but think about how it might decide and elaborate on some of those thoughts here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1150fbb",
   "metadata": {},
   "source": [
    "# Correlation between model parameters\n",
    "\n",
    "In last week's notebook, you may have noted that we set t=0 to be in the middle of 2014, which might seem arbitrary. We've reproduced the plot below as a reference to remind you about what the data looked like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde23d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.errorbar(years_since_mid_2014, excess_counts, \n",
    "            fmt='ko', ms=4, ecolor='pink', yerr=sigma_counts)\n",
    "\n",
    "ax.set_xlabel(r\"Time since mid 2014 [years]\")\n",
    "ax.set_ylabel(r\"$n_{\\rm excess}$ [per week]\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8ad86",
   "metadata": {},
   "source": [
    "There was actually a very good reason to do that, which we will examine now. \n",
    "\n",
    "The Fermi telescope actually launched in 2008, so we could set t=0 to be in January 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_since_2008 = date_YEAR  - 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making the same plot, but with our new x-axis values\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.errorbar(years_since_2008, excess_counts, \n",
    "            fmt='ko', ms=4, ecolor='pink', yerr=sigma_counts)\n",
    "\n",
    "ax.set_xlabel(r\"Time since 2008 [years]\")\n",
    "ax.set_ylabel(r\"$n_{\\rm excess}$ [per week]\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c3386",
   "metadata": {},
   "source": [
    "Now, let's think about what happens to fitting our linear model when we make this change. \n",
    "\n",
    "Keep in mind that the model function we are using is a simple equation for a line: $y = p_0 + p_1 x$\n",
    "\n",
    "By moving the zero point of the x-axis, we are changing how that function describes the data, essentially adding a horizontal (time) offset to the $x$ points. The model parameters are now more *correlated*. To demonstrate this, we will make a plot for different values of $p_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start by making the same plot as before with our excess counts and \n",
    "## associated uncertainties, but now plotted with a different x-axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.errorbar(years_since_2008, excess_counts, \n",
    "            alpha=0.2, yerr=sigma_counts)\n",
    "\n",
    "ax.set_xlabel(r\"Time since 2008 [years]\")\n",
    "ax.set_ylabel(r\"$n_{\\rm excess}$ [per week]\")\n",
    "\n",
    "## Plot the linear model for a few different values of the slope parameter\n",
    "## i.e. parameter p1, i.e. params[1], all with a constant offset of 0\n",
    "xvals = years_since_2008\n",
    "params = np.array([0, 0])\n",
    "for slope in np.linspace(-15, 15, 5):\n",
    "    params[1] = slope\n",
    "    ax.plot(xvals, linear_function(xvals, params), lw=2,\n",
    "            label=rf\"Slope = {slope:0.1f}\", zorder=5)\n",
    "\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cd9f3",
   "metadata": {},
   "source": [
    "As you can see, all the lines cross at t=0, which is now off the left side of the plot.  Before it was more or less in the middle of the plot.  \n",
    "\n",
    "**What this means, is that if you were to pick a value like $p_1 = 7.5$, the model tends to be above the average of the data for the entire time.  This means that you would have to change the offset $p_0$ to a negative number to compensate.**\n",
    "\n",
    "That is exactly what we mean when we say that the parameters have become more correlated: a change in one parameter necessitates a change in another parameter in order to keep the model consistent with the data. \n",
    "\n",
    "Let's explore this effect with the minimizer.\n",
    "\n",
    "First we have to make a version of the cost function that uses this version of the x-axis data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75531a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_bad(params):\n",
    "    return 0.5*chi2_function(years_since_2008, excess_counts, sigma_counts, linear_function, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bcaca",
   "metadata": {},
   "source": [
    "Now, let's minimize the cost function again to find the best fit parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Minimize our new cost function, starting at (0, 0)\n",
    "result_bad = optimize.minimize(cost_function_bad, [0., 0.])\n",
    "\n",
    "## Extract the best fit parameters and the value of the cost \n",
    "## function at the minimum. \n",
    "pars_bad = result_bad['x']\n",
    "fmin_bad = result_bad['fun']\n",
    "p0_best_bad = pars_bad[0]\n",
    "p1_best_bad = pars_bad[1]\n",
    "\n",
    "## Extract the covariance in parameters, as estimated from the\n",
    "## inverse of the Hessian matrix.\n",
    "cov_bad = result_bad['hess_inv']\n",
    "\n",
    "## Compute uncertainties as the square root of the diagonal\n",
    "## elements of the covariance matrix, and then compute the\n",
    "## correlation of the parameters based on these values\n",
    "p0_err_bad = np.sqrt(cov_bad[0,0])\n",
    "p1_err_bad = np.sqrt(cov_bad[1,1])\n",
    "correl_bad = cov_bad[1,0]/(p0_err_bad*p1_err_bad)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Human readable version: 'bad' idea ---------------\")\n",
    "print(f\"  Minimum value of cost function: {fmin_bad:.1f}\")\n",
    "print(f\"         Minimum value of chi**2: {(2*fmin_bad):.1f}\")\n",
    "print(f\"                     p0 best fit: {p0_best_bad:.1f} ± {p0_err_bad:.1f} counts\")\n",
    "print(f\"                     p1 best fit: {p1_best_bad:.1f} ± {p1_err_bad:.1f} counts / year\")\n",
    "print(f\"   Correlation between p0 and p1: {correl_bad:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad674a",
   "metadata": {},
   "source": [
    "Let's make a similar 2D colormap that we made before with our manual scan, but now using the \"bad\" cost function.\n",
    "\n",
    "First, we'll generate the scan points to make the colormap itself, then we'll plot the minimized result on top. For this step, we've increased the number of scan points from 21->51, to improve the smoothness of the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee1cc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Alternate number of points to scan in this 2D implementation\n",
    "nx = 51\n",
    "ny = 51\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_2d_scan_vals_bad = np.zeros((nx, ny))\n",
    "\n",
    "## Scan the offset between -10 and 10, and the slope between -5 and 5\n",
    "offset_2d_scan_points = np.linspace(-10, 10, nx)\n",
    "slope_2d_scan_points = np.linspace(-5, 5, ny)\n",
    "\n",
    "## Double loop for 2d scan, sampling the value of chi^2 at each combination of \n",
    "## parameter values: offset and slope\n",
    "for i in range(nx):\n",
    "    params[0] = offset_2d_scan_points[i]\n",
    "    for j in range(ny):\n",
    "        params[1] = slope_2d_scan_points[j]\n",
    "\n",
    "        ## Scan the \"bad\" values simply by using the \"years_since_2008\" array\n",
    "        ## instead of the \"years_since_mid_2014\" array.\n",
    "        chi2_2d_scan_vals_bad[i,j] = chi2_function(years_since_2008, excess_counts, sigma_counts,\n",
    "                                               linear_function, params)\n",
    "\n",
    "## Subtract the minimum chi2 value to get the \"delta chi^2\"\n",
    "min_chi2_bad = chi2_2d_scan_vals_bad.min()\n",
    "chi2_2d_scan_vals_bad -= min_chi2_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e8899",
   "metadata": {},
   "source": [
    "Plotting our minimized result together with the \"BAD\" 2D scan,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's plot our \"BAD\" data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "## Plot the chi^2 values as a 2D image, with the color scale \n",
    "## indicating the value of chi^2\n",
    "img_bad = ax.imshow(chi2_2d_scan_vals_bad.T, extent=(-10, 10, -5, 5), origin='lower', aspect='auto', cmap='cividis')\n",
    "fig.colorbar(img_bad, label=r\"$\\Delta \\chi^2$\")\n",
    "\n",
    "## Plot some contours associated to different values of delta chi^2, \n",
    "## and thus different uncertainties on the computed parameters. We'll \n",
    "## plot contours for 1, 4, and 9, which correspond to 1, 2, and 3 \n",
    "## sigma uncertainties\n",
    "ax.contour(offset_2d_scan_points, slope_2d_scan_points, chi2_2d_scan_vals_bad.T, levels=[1, 4, 9], colors=\"white\")\n",
    "\n",
    "## Plot the best-fit from our bad scan\n",
    "plt.errorbar(p0_best_bad, p1_best_bad, xerr=p0_err_bad, yerr=p1_err_bad, color='cyan')\n",
    "\n",
    "ax.set_xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "ax.set_ylabel(r\"$p_1$ = Slope [counts/year]\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## Print some comparisons of our two fits\n",
    "print(f\" New best fit value is {min_chi2_bad:0.1f} for ({p0_best_bad:+0.1f} ± {p0_err_bad:0.1f}, \" \n",
    "        + f\"{p1_best_bad:+0.1f} ± {p1_err_bad:0.1f})\" )\n",
    "print(f\"Original fit value was {min_chi2:0.1f} for ({p0_best:+0.1f} ± {p0_err:0.1f}, \" \n",
    "        + f\"{p1_best:+0.1f} ± {p1_err:0.1f})\", end='\\n\\n')\n",
    "print(f\"Original correlation was {correl:+0.2f}, now it is {correl_bad:+0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e153863f",
   "metadata": {},
   "source": [
    "### Questions for discussion\n",
    "\n",
    "#### 3.1 What is going on in this plot? Why are the contours tilted? Why are the error bars larger? Explain your answers to these questions in some amount of detail.\n",
    "\n",
    "#### 3.2 What does this tell us about what we should consider when building models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kipac_physics89L",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
